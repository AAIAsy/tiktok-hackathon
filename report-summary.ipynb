{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28366ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c437997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"research-papers\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efa1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=307\n",
    ")\n",
    "docs = text_splitter.split_documents(documents=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613947e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    text_chunk.append(docs[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327529d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n",
      "Rate limited, waiting 5 seconds...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_text_embedding(input):\n",
    "    embeddings_batch_response = client.embeddings.create(\n",
    "        model=\"mistral-embed\",\n",
    "        inputs=[input]  # Still send as list for consistency\n",
    "    )\n",
    "    return embeddings_batch_response.data[0].embedding\n",
    "\n",
    "def get_embeddings_with_rate_limit(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            embedding = get_text_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "            time.sleep(0.1)  # Small delay between requests\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e):\n",
    "                print(\"Rate limited, waiting 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "                embedding = get_text_embedding(chunk)  # Retry\n",
    "                embeddings.append(embedding)\n",
    "            else:\n",
    "                raise e\n",
    "    return np.array(embeddings)\n",
    "\n",
    "text_embeddings = get_embeddings_with_rate_limit(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a83e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0950df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the necessary and most accurate feature selction techniques to identify spam?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d84173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=50) # distance, index\n",
    "retrieved_chunk = [text_chunk[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb3f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ed71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the most necessary and accurate **feature selection techniques** to identify spam reviews involve a combination of **feature engineering approaches** and **selection methods** that address the unique challenges of review spam detection (e.g., high dimensionality, class imbalance, and noisy data). Here’s a summary of the key techniques and insights:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Feature Engineering Approaches**\n",
      "The accuracy of spam detection heavily depends on the **types of features extracted**. The most effective features include:\n",
      "- **Linguistic Features**:\n",
      "  - **N-grams**: Unigrams, bigrams, or trigrams (e.g., word pairs like \"amazing product\").\n",
      "  - **Lexical Features**: Part-of-speech (POS) tags, punctuation patterns, or function words (e.g., excessive use of adjectives like \"incredible\").\n",
      "  - **Stylometric Features**: Writing style metrics (e.g., average sentence length, vocabulary richness).\n",
      "  - **Semantic Features**: Sentiment analysis (e.g., overly positive/negative language) or semantic similarity between reviews.\n",
      "  - **LIWC (Linguistic Inquiry and Word Count)**: Psycholinguistic features (e.g., emotional tone, cognitive words).\n",
      "\n",
      "- **Reviewer-Centric (Behavioral) Features**:\n",
      "  - **Activity Patterns**: Number of reviews per day, burstiness (sudden spikes in activity), or review frequency.\n",
      "  - **Deviation from Norms**: Rating deviation (e.g., consistently 5-star reviews for a product with average 3-star ratings).\n",
      "  - **Metadata**: Reviewer ID, IP address, geographical location, or verified purchase status (e.g., Amazon Verified Purchase).\n",
      "  - **Temporal Features**: Time between reviews or review posting patterns (e.g., reviews posted at unusual hours).\n",
      "\n",
      "- **Review Content Features**:\n",
      "  - **Duplication**: Similarity to other reviews (e.g., near-duplicate text).\n",
      "  - **Length**: Spam reviews are often shorter or longer than genuine reviews.\n",
      "  - **Sentiment Polarity**: Overly positive/negative sentiment (e.g., fake reviews tend to be extremist).\n",
      "\n",
      "- **Hybrid Features**:\n",
      "  Combining **textual features** (e.g., n-grams) with **behavioral features** (e.g., reviewer activity) yields the highest accuracy. For example:\n",
      "  - **Mukherjee et al. (2013)** achieved **86.1% accuracy** on Yelp’s dataset using **behavioral + bigram features**.\n",
      "  - **Ott et al.** found that **behavioral features** (e.g., reviewer history) outperformed **n-gram features** alone on real-world data.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Feature Selection Techniques**\n",
      "Due to the **high dimensionality** of text data (thousands of n-grams), feature selection is critical to improve performance and reduce computational cost. The most effective techniques include:\n",
      "- **Information Gain (IG)**:\n",
      "  - Ranks features by their ability to distinguish between spam/non-spam.\n",
      "  - Used in **Mukherjee et al. (2013)** to select the top 1–2% features, though it showed **no improvement** in their study (likely due to limited testing).\n",
      "  - **Recommendation**: Test IG with different feature subset sizes (e.g., top 5%, 10%).\n",
      "\n",
      "- **Gini Index**:\n",
      "  - Measures feature discriminative power (higher Gini = better feature).\n",
      "  - Useful for **decision tree-based classifiers**.\n",
      "\n",
      "- **χ² (Chi-Square) Statistic**:\n",
      "  - Tests independence between features and class labels (spam/non-spam).\n",
      "  - Effective for **lexical features** (e.g., words strongly associated with spam).\n",
      "\n",
      "- **Term Frequency-Inverse Document Frequency (TF-IDF)**:\n",
      "  - Weights features by importance (rare but discriminative words get higher scores).\n",
      "  - Helps reduce noise from common words (e.g., \"the\", \"and\").\n",
      "\n",
      "- **Embedding-Based Methods** (e.g., Word2Vec, BERT):\n",
      "  - Captures semantic relationships between words (e.g., \"fake\" and \"paid\" may cluster together).\n",
      "  - Reduces dimensionality while preserving meaning.\n",
      "\n",
      "- **Ensemble Feature Selection**:\n",
      "  - Combine multiple methods (e.g., IG + χ²) to select robust features.\n",
      "\n",
      "---\n",
      "### **3. Addressing Challenges**\n",
      "#### **Class Imbalance**\n",
      "- Fake reviews are **rare** (e.g., <5% of datasets). Techniques to mitigate imbalance:\n",
      "  - **Random Oversampling (ROS)**: Duplicate minority class (spam) samples.\n",
      "  - **Random Undersampling (RUS)**: Reduce majority class (non-spam) samples.\n",
      "  - **Synthetic Sampling (SMOTE)**: Generate synthetic spam examples.\n",
      "  - **Example**: A study achieved **99.59% F-measure** using **ROS + Naïve Bayes**.\n",
      "\n",
      "#### **Noisy Data**\n",
      "- Real-world datasets often have **mislabelled reviews** (e.g., human judges struggle to distinguish spam).\n",
      "  - **Ensemble Learning** (e.g., Bagging, Boosting):\n",
      "    - Combines multiple classifiers (e.g., Random Forest, AdaBoost) to improve robustness.\n",
      "    - **Boosting** (e.g., XGBoost) is particularly effective for imbalanced data.\n",
      "  - **Unsupervised Methods**:\n",
      "    - Clustering (e.g., K-means) to detect anomalies (spam as outliers).\n",
      "\n",
      "#### **High Dimensionality**\n",
      "- **Dimensionality Reduction**:\n",
      "  - **Principal Component Analysis (PCA)**: Projects features into lower-dimensional space.\n",
      "  - **Autoencoders**: Neural networks for non-linear feature compression.\n",
      "- **Distributed Computing**:\n",
      "  - Tools like **Apache Spark (MLlib)** or **SAMOA** for scalable feature selection on big data.\n",
      "\n",
      "---\n",
      "### **4. Most Accurate Combinations (From Literature)**\n",
      "| **Study**               | **Features Used**                          | **Classifier**       | **Accuracy** | **Dataset**          |\n",
      "|-------------------------|-------------------------------------------|----------------------|--------------|----------------------|\n",
      "| Mukherjee et al. (2013) | Behavioral + Bigrams                     | SVM                  | 86.1%        | Yelp (real-world)    |\n",
      "| Ott et al. (2011)       | Unigrams/Bigrams + LIWC                   | SVM                  | 89.8%        | AMT (synthetic)      |\n",
      "| Jindal et al. (2008)    | Reviewer-centric + N-grams                | Decision Tree        | 78%          | Amazon               |\n",
      "| Hammad (2016)           | Lexical + Behavioral (Arabic reviews)     | Naïve Bayes (ROS)    | 99.59% F1    | TripAdvisor          |\n",
      "\n",
      "---\n",
      "### **5. Key Recommendations**\n",
      "1. **Use Hybrid Features**:\n",
      "   - Combine **textual** (n-grams, LIWC), **behavioral** (reviewer activity), and **sentiment** features.\n",
      "2. **Leverage Ensemble Methods**:\n",
      "   - **Boosting (XGBoost, AdaBoost)** or **Bagging (Random Forest)** to handle noise and imbalance.\n",
      "3. **Apply Feature Selection**:\n",
      "   - Test **Information Gain**, **χ²**, or **TF-IDF** to reduce dimensionality.\n",
      "   - Avoid relying solely on **synthetic datasets** (e.g., AMT); validate on **real-world data** (e.g., Yelp, Amazon).\n",
      "4. **Address Class Imbalance**:\n",
      "   - Use **SMOTE** or **ensemble sampling** (e.g., EasyEnsemble).\n",
      "5. **Explore Unsupervised Learning**:\n",
      "   - For unlabeled data, use **clustering** (e.g., DBSCAN) or **anomaly detection** (e.g., Isolation Forest).\n",
      "6. **Scalability**:\n",
      "   - Use **distributed frameworks** (Spark, H2O) for large-scale datasets.\n",
      "\n",
      "---\n",
      "### **6. Future Directions**\n",
      "- **Deep Learning**:\n",
      "  - **BERT** or **Transformer-based models** for contextual feature extraction.\n",
      "- **Graph-Based Features**:\n",
      "  - Model reviewer-review relationships (e.g., spammers in the same network).\n",
      "- **Streaming Feature Selection**:\n",
      "  - Adaptive methods for **real-time spam detection** (e.g., online learning with SAMOA).\n",
      "\n",
      "---\n",
      "### **Conclusion**\n",
      "The **most accurate feature selection techniques** for spam detection involve:\n",
      "1. **Hybrid feature sets** (text + behavioral).\n",
      "2. **Information Gain/χ²** for dimensionality reduction.\n",
      "3. **Ensemble classifiers** (e.g., XGBoost) to handle imbalance/noise.\n",
      "4. **Validation on real-world datasets** (e.g., Yelp, Amazon).\n",
      "\n",
      "No single \"silver bullet\" exists, but combining these techniques yields the highest accuracy. Future work should focus on **scalable, adaptive methods** (e.g., deep learning + streaming feature selection).\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\", max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "            chat_response = client.chat.complete(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            return chat_response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) and attempt < max_retries - 1:\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"Rate limited. Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "result = run_mistral(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
