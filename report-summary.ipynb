{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28366ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai import Mistral\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c437997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "loader = PyPDFDirectoryLoader(\"research-papers\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2efa1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2048,\n",
    "    chunk_overlap=307\n",
    ")\n",
    "docs = text_splitter.split_documents(documents=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613947e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = []\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    text_chunk.append(docs[i].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327529d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 2 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 3 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 4 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 5 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 6 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 7 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 8 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 9 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 10 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 11 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 12 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 13 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 14 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 15 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 16 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 17 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 18 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 19 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 20 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 21 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 22 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 23 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 24 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 25 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 26 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 27 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 28 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 29 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 30 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 31 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 32 (3 chunks)...\n",
      "Waiting between batches...\n",
      "Processing batch 33 (2 chunks)...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def get_text_embedding(input, max_retries=3):\n",
    "    \"\"\"Get embedding with built-in retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            embeddings_batch_response = client.embeddings.create(\n",
    "                model=\"mistral-embed\",\n",
    "                inputs=[input]\n",
    "            )\n",
    "            return embeddings_batch_response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) and attempt < max_retries - 1:\n",
    "                # Exponential backoff with jitter\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 2)\n",
    "                print(f\"Rate limited on attempt {attempt + 1}. Waiting {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                # If it's the last attempt or not a rate limit error, raise it\n",
    "                raise e\n",
    "\n",
    "def get_embeddings_with_rate_limit(chunks, base_delay=0.5):\n",
    "    \"\"\"Process embeddings with rate limiting and progress tracking\"\"\"\n",
    "    embeddings = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            print(f\"Processing chunk {i+1}/{total_chunks}...\")\n",
    "            embedding = get_text_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "            \n",
    "            # Progressive delay - increase delay if we're processing many chunks\n",
    "            if i < total_chunks - 1:  # Don't sleep after the last chunk\n",
    "                delay = base_delay + (i * 0.1)  # Gradually increase delay\n",
    "                time.sleep(min(delay, 2.0))  # Cap at 2 seconds\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get embedding for chunk {i+1}: {str(e)}\")\n",
    "            # You can choose to either skip this chunk or stop processing\n",
    "            # For now, let's skip and continue\n",
    "            print(\"Skipping this chunk and continuing...\")\n",
    "            continue\n",
    "    \n",
    "    return np.array(embeddings) if embeddings else np.array([])\n",
    "\n",
    "# Alternative version with batch processing (if your chunks are small)\n",
    "def get_embeddings_batch_safe(chunks, batch_size=5, max_retries=3):\n",
    "    \"\"\"Process embeddings in smaller batches to reduce rate limiting\"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} ({len(batch)} chunks)...\")\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Process batch with longer delay between batches\n",
    "                batch_embeddings = []\n",
    "                for chunk in batch:\n",
    "                    embedding = get_text_embedding(chunk, max_retries=2)\n",
    "                    batch_embeddings.append(embedding)\n",
    "                    time.sleep(0.3)  # Small delay within batch\n",
    "                \n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                break  # Success, exit retry loop\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e) and attempt < max_retries - 1:\n",
    "                    wait_time = 10 + (attempt * 5)  # 10, 15, 20 seconds\n",
    "                    print(f\"Batch rate limited. Waiting {wait_time} seconds before retrying...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Failed to process batch after {max_retries} attempts: {str(e)}\")\n",
    "                    raise e\n",
    "        \n",
    "        # Longer delay between batches\n",
    "        if i + batch_size < len(chunks):\n",
    "            print(\"Waiting between batches...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Usage examples:\n",
    "# Option 1: Simple with better retry logic\n",
    "# text_embeddings = get_embeddings_with_rate_limit(text_chunk)\n",
    "\n",
    "# Option 2: Batch processing (recommended for large datasets)\n",
    "text_embeddings = get_embeddings_batch_safe(text_chunk, batch_size=3)\n",
    "\n",
    "# Option 3: Very conservative approach\n",
    "# text_embeddings = get_embeddings_with_rate_limit(text_chunk, base_delay=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a83e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in FAISS\n",
    "\n",
    "import faiss\n",
    "\n",
    "d = text_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0950df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"The papers disuss unsupervised learning strategies and NLP techniques to extract features of reviews. What are the necessary and most accurate feature extraction and selection techniques to identify spam or not spam, based on the paper?\"\n",
    "question_embeddings = np.array([get_text_embedding(question)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d84173d",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, I = index.search(question_embeddings, k=50) # distance, index\n",
    "retrieved_chunk = [text_chunk[i] for i in I.tolist()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ed71d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate limited. Retrying in 1.21 seconds...\n",
      "Based on the provided context, the most **necessary and accurate feature extraction and selection techniques** for identifying spam or non-spam reviews, combining **unsupervised learning strategies** and **NLP techniques**, are summarized below:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Feature Extraction Techniques**\n",
      "#### **A. Linguistic (NLP-Based) Features**\n",
      "These are the most dominant and effective techniques for spam detection, focusing on the **textual content** of reviews. Key steps include:\n",
      "\n",
      "1. **Preprocessing**:\n",
      "   - **Stopword Removal**: Eliminate irrelevant words (e.g., \"the,\" \"is,\" \"and\").\n",
      "   - **Punctuation Removal**: Clean text by removing symbols.\n",
      "   - **Stemming/Lemmatization**: Reduce words to their root form (e.g., \"working\" → \"work\").\n",
      "   - **Part-of-Speech (POS) Tagging**: Label words as nouns, verbs, adjectives, etc., to capture syntactic patterns.\n",
      "\n",
      "2. **Tokenization**:\n",
      "   - **N-grams**:\n",
      "     - **Unigrams** (single words, e.g., \"good\").\n",
      "     - **Bigrams** (pairs, e.g., \"good car\").\n",
      "     - **Trigrams** (triplets, e.g., \"very good car\").\n",
      "   - **Combination of N-grams** (e.g., uni + bi-grams) often improves accuracy.\n",
      "\n",
      "3. **Transformation**:\n",
      "   - **Document-Term Matrix (DTM)**: Represent reviews as sparse matrices of word frequencies.\n",
      "   - **TF-IDF (Term Frequency-Inverse Document Frequency)**: Weight words by importance (common words like \"the\" get lower weights).\n",
      "   - **Bag-of-Words (BoW)**: Simplest representation using word counts.\n",
      "\n",
      "4. **Feature Selection**:\n",
      "   - **Information Gain**: Measures how well a word discriminates between spam/non-spam (higher gain = more relevant).\n",
      "   - **χ²-Statistic**: Identifies words with strong dependence on class labels (spam/non-spam).\n",
      "   - **Mutual Information**: Captures statistical dependence between words and classes.\n",
      "\n",
      "#### **B. Spammer Behavioral Features**\n",
      "These focus on **metadata and reviewer patterns**, often used alongside linguistic features:\n",
      "   - **Reviewer-Centric**:\n",
      "     - **Review Frequency**: Spammers often post multiple reviews in short time spans.\n",
      "     - **Deviation from Average Rating**: Spammers may give extreme ratings (e.g., all 5-star).\n",
      "     - **Review Length**: Spam reviews are often shorter or longer than genuine ones.\n",
      "     - **Temporal Patterns**: Time between reviews (e.g., multiple reviews in <1 minute).\n",
      "     - **IP/Geolocation**: Multiple reviews from the same IP/location.\n",
      "   - **Review-Centric**:\n",
      "     - **Content Similarity**: Duplicate or near-identical reviews across products.\n",
      "     - **Sentiment Polarization**: Overuse of extreme words (\"amazing,\" \"terrible\").\n",
      "     - **Helpfulness Votes**: Low \"helpful\" votes may indicate spam.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Unsupervised Learning Strategies**\n",
      "For scenarios with **unlabeled data**, the following techniques are effective:\n",
      "\n",
      "#### **A. Clustering-Based Methods**\n",
      "   - **K-Means Clustering**:\n",
      "     - Groups reviews into clusters based on feature similarity (e.g., TF-IDF vectors).\n",
      "     - Works well for large datasets; accuracy depends on choosing optimal *K* (number of clusters).\n",
      "     - Example: Achieved **71% precision** in detecting spam in Chinese reviews (Jia et al.).\n",
      "   - **Twice-Clustering**:\n",
      "     - First clusters the dataset, then applies sub-clustering to improve precision.\n",
      "     - Reported **66% accuracy** on product reviews (360buy.com dataset).\n",
      "   - **Constrained K-Means (COP-K-Means)**:\n",
      "     - Incorporates domain constraints (e.g., reviewer behavior) to guide clustering.\n",
      "\n",
      "#### **B. Anomaly Detection**\n",
      "   - **Deviation-Based Methods**:\n",
      "     - **Aspect-Based Review Deviation**: Detects reviews deviating from the norm for a product aspect (e.g., \"battery life\").\n",
      "     - **Latent Content Deviation**: Uses topic modeling (e.g., LDA) to identify outliers.\n",
      "     - Example: Achieved **78.15% accuracy** on Amazon reviews.\n",
      "\n",
      "#### **C. Lexicon-Based Techniques**\n",
      "   - **Dictionary/Corpus-Based**:\n",
      "     - Compares review text against precompiled sentimental lexicons (e.g., lists of positive/negative words).\n",
      "     - **Corpus-Based**: Better accuracy than dictionary methods (Table 14 in the context).\n",
      "   - **Rule-Based Systems**:\n",
      "     - Uses heuristic rules (e.g., \"reviews posted within 1 minute = spam\").\n",
      "     - Example: **65% accuracy** for emotion detection in micro-blogs (Gao et al.).\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Most Accurate Combinations**\n",
      "The papers highlight that **combining multiple techniques** yields the highest accuracy:\n",
      "1. **Linguistic + Behavioral Features**:\n",
      "   - Example: **89.6% accuracy** using bi-grams + reviewer features (Hotel review dataset via AMT).\n",
      "   - **86.1% accuracy** with behavioral features + bi-grams (Yelp dataset).\n",
      "2. **Supervised + Unsupervised Hybrid**:\n",
      "   - Use clustering (e.g., K-means) to label data, then apply supervised methods (e.g., SVM).\n",
      "3. **Ensemble Methods**:\n",
      "   - Combine outputs from multiple models (e.g., SVM + Random Forest) to improve robustness.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Performance Metrics for Evaluation**\n",
      "To validate accuracy, the papers emphasize:\n",
      "   - **Precision/Recall/F1-Score**: Balance between false positives/negatives.\n",
      "   - **AUC-ROC**: Measures classifier performance across thresholds.\n",
      "   - **Accuracy**: Overall correct classification rate (e.g., **91% with Random Forest** on Amazon data).\n",
      "   - **Lift Curve**: Visualizes model performance for imbalanced datasets.\n",
      "\n",
      "---\n",
      "\n",
      "### **5. Key Insights from the Papers**\n",
      "- **Linguistic features alone** (e.g., n-grams + TF-IDF) are **highly effective** but benefit from adding behavioral features.\n",
      "- **Unsupervised methods** (e.g., clustering) are useful for **unlabeled data** but generally **less accurate** than supervised approaches.\n",
      "- **Real-world datasets** (e.g., Yelp, Amazon) pose challenges due to noise; **synthetic datasets** (e.g., AMT) may overestimate accuracy.\n",
      "- **Feature selection** (e.g., information gain) is critical to avoid overfitting and improve efficiency.\n",
      "\n",
      "---\n",
      "### **Recommended Pipeline for Spam Detection**\n",
      "1. **Preprocess Text**: Clean, tokenize, and apply POS tagging/stemming.\n",
      "2. **Extract Features**:\n",
      "   - Linguistic: N-grams (uni/bi), TF-IDF, sentiment lexicons.\n",
      "   - Behavioral: Reviewer frequency, rating deviation, temporal patterns.\n",
      "3. **Feature Selection**: Use χ² or information gain to retain top features.\n",
      "4. **Model Training**:\n",
      "   - **Supervised**: SVM, Random Forest, or Logistic Regression (if labeled data exists).\n",
      "   - **Unsupervised**: K-means or anomaly detection (for unlabeled data).\n",
      "5. **Evaluate**: Use AUC-ROC, precision/recall, and accuracy metrics.\n",
      "\n",
      "---\n",
      "### **Example from the Papers**\n",
      "- **Highest Accuracy (91%)**: Random Forest with **reviewer + product features** on Amazon dataset ([53]).\n",
      "- **Best Unsupervised (78.15%)**: Latent content deviation on Amazon reviews ([63]).\n",
      "- **Lexicon-Based (89.9%)**: SVM with bi-grams on AMT hotel reviews ([15]).\n",
      "\n",
      "By integrating these techniques, practitioners can build **robust spam detection systems** tailored to their datasets.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{retrieved_chunk}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def run_mistral(user_message, model=\"mistral-large-latest\", max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "            chat_response = client.chat.complete(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            return chat_response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) and attempt < max_retries - 1:\n",
    "                wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                print(f\"Rate limited. Retrying in {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "result = run_mistral(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
