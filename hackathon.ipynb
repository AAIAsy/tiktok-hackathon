{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04052a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 1: Clean the 'resp.text' field (Business Responses)\n",
    "# # =====================================================\n",
    "# df_review = df_review.withColumn(\"resp_text\", F.col(\"resp.text\"))\n",
    "# # remove (Translated by Google)\n",
    "# df_review = df_review.withColumn(\"resp_text\", F.regexp_replace(\"resp_text\", r'\\(Translated by Google\\)', ''))\n",
    "# # remove (Original) and everything after\n",
    "# df_review = df_review.withColumn(\"resp_text\", F.regexp_replace(\"resp_text\", r'\\(Original\\).*', ''))\n",
    "# # replace newlines with spaces\n",
    "# df_review = df_review.withColumn(\"resp_text\", F.regexp_replace(\"resp_text\", r'[\\r\\n]+', ' '))\n",
    "# # trim extra spaces\n",
    "# df_review = df_review.withColumn(\"resp_text\", F.trim(\"resp_text\"))\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 2: Clean the 'text' column (Customer Reviews)\n",
    "# # =====================================================\n",
    "# df_review = df_review.withColumn(\"review_text\", F.col(\"text\"))\n",
    "# # remove (Translated by Google) if present\n",
    "# df_review = df_review.withColumn(\"review_text\", F.regexp_replace(\"review_text\", r'\\(Translated by Google\\)', ''))\n",
    "# # remove (Original) and everything after\n",
    "# df_review = df_review.withColumn(\"review_text\", F.regexp_replace(\"review_text\", r'\\(Original\\).*', ''))\n",
    "# # replace newlines with spaces\n",
    "# df_review = df_review.withColumn(\"review_text\", F.regexp_replace(\"review_text\", r'[\\r\\n]+', ' '))\n",
    "# # trim extra spaces\n",
    "# df_review = df_review.withColumn(\"review_text\", F.trim(\"review_text\"))\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 3: Tokenization for Customer Reviews\n",
    "# # =====================================================\n",
    "# review_tokenizer = RegexTokenizer(inputCol=\"review_text\", outputCol=\"review_tokens\", pattern=\"\\\\W\")\n",
    "# df_tokens = review_tokenizer.transform(df_review)\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 4: Tokenization for Business Responses\n",
    "# # =====================================================\n",
    "# resp_tokenizer = RegexTokenizer(inputCol=\"resp_text\", outputCol=\"resp_tokens\", pattern=\"\\\\W\")\n",
    "# df_tokens = resp_tokenizer.transform(df_tokens)\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 5: Stopword Removal for Customer Reviews\n",
    "# # =====================================================\n",
    "# review_remover = StopWordsRemover(inputCol=\"review_tokens\", outputCol=\"review_filtered_tokens\")\n",
    "# df_clean = review_remover.transform(df_tokens)\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 6: Stopword Removal for Business Responses\n",
    "# # =====================================================\n",
    "# resp_remover = StopWordsRemover(inputCol=\"resp_tokens\", outputCol=\"resp_filtered_tokens\")\n",
    "# df_clean = resp_remover.transform(df_clean)\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 7: TF-IDF for Customer Reviews\n",
    "# # =====================================================\n",
    "# from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# # HashingTF for customer reviews\n",
    "# review_hashingTF = HashingTF(inputCol=\"review_filtered_tokens\", outputCol=\"review_rawFeatures\")\n",
    "# review_featurized = review_hashingTF.transform(df_clean)\n",
    "\n",
    "# # IDF for customer reviews\n",
    "# review_idf = IDF(inputCol=\"review_rawFeatures\", outputCol=\"review_features\")\n",
    "# review_idfModel = review_idf.fit(review_featurized)\n",
    "# df_with_review_features = review_idfModel.transform(review_featurized)\n",
    "\n",
    "# # =====================================================\n",
    "# # STEP 8: TF-IDF for Business Responses\n",
    "# # =====================================================\n",
    "# # HashingTF for business responses\n",
    "# resp_hashingTF = HashingTF(inputCol=\"resp_filtered_tokens\", outputCol=\"resp_rawFeatures\")\n",
    "# resp_featurized = resp_hashingTF.transform(df_with_review_features)\n",
    "\n",
    "# # IDF for business responses\n",
    "# resp_idf = IDF(inputCol=\"resp_rawFeatures\", outputCol=\"resp_features\")\n",
    "# resp_idfModel = resp_idf.fit(resp_featurized)\n",
    "# df_final = resp_idfModel.transform(resp_featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206f38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a8cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "590f22ef",
   "metadata": {},
   "source": [
    "# Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9c6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, concat_ws, rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fb8052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/28 00:31:18 WARN Utils: Your hostname, Asyrafs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.18.78 instead (on interface en0)\n",
      "25/08/28 00:31:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/28 00:31:18 WARN Utils: Your hostname, Asyrafs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.18.78 instead (on interface en0)\n",
      "25/08/28 00:31:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/28 00:31:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/28 00:31:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Hackathon\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "pathing_review = \"datasets/review_data/\"\n",
    "arr = np.array(os.listdir(pathing_review))\n",
    "reviewData_files = pathing_review + arr\n",
    "\n",
    "pathing_metadata = \"datasets/review_metadata/\"\n",
    "arr = np.array(os.listdir(pathing_metadata))\n",
    "reviewMetadata_files = pathing_metadata + arr\n",
    "\n",
    "df_review = spark.read.json(list(reviewData_files)).dropna(subset=\"text\").drop_duplicates()\n",
    "df_metadata = spark.read.json(list(reviewMetadata_files)).dropna(subset=\"category\").drop_duplicates().select([\"gmap_id\", \"category\"])\n",
    "\n",
    "df_joined = df_review.join(df_metadata, on=\"gmap_id\", how=\"inner\").withColumn(\"category_str\", concat_ws(\", \", col(\"category\"))).withColumn(\"random_order\", rand()).orderBy(\"random_order\").drop(\"random_order\").limit(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pics: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- resp: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- time: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- category_str: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You must classify each review into exactly ONE of these labels. Respond with ONLY the label text, nothing else.\n",
    "\n",
    "Valid labels (respond with exactly this text):\n",
    "- Advertisement\n",
    "- Irrelevant content\n",
    "- Rant without visit\n",
    "- Low quality review\n",
    "- Acceptable review\n",
    "\n",
    "Classification rules:\n",
    "1. Advertisement: Contains promotional content, links, or sales pitches\n",
    "2. Irrelevant content: Not about the business location (personal stories, unrelated topics)\n",
    "3. Rant without visit: Complaints from someone who clearly hasn't visited\n",
    "4. Low quality review: Very short, just emojis, or provides no useful information\n",
    "5. Acceptable review: Genuine experience at the location with useful details\n",
    "\n",
    "Business category: %s\n",
    "\n",
    "IMPORTANT: Respond with ONLY one of the five labels above. Do not include numbers, prefixes, or explanations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd74021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_batch = df_joined.withColumn(\n",
    "    \"json_request\",\n",
    "    F.to_json(F.struct(\n",
    "        F.concat(F.lit(\"request-\"), F.monotonically_increasing_id()).alias(\"custom_id\"),\n",
    "        F.lit(\"POST\").alias(\"method\"),\n",
    "        F.lit(\"/v1/chat/completions\").alias(\"url\"),\n",
    "        F.struct(\n",
    "            F.lit(\"gpt-4o-mini\").alias(\"model\"),  # Use a more standard model\n",
    "            F.array(\n",
    "                F.struct(\n",
    "                    F.lit(\"system\").alias(\"role\"),\n",
    "                    F.format_string(system_prompt, F.col(\"category_str\")).alias(\"content\")\n",
    "                ),\n",
    "                F.struct(\n",
    "                    F.lit(\"user\").alias(\"role\"),\n",
    "                    F.concat(F.lit(\"Review: \"), F.col(\"text\")).alias(\"content\")\n",
    "                )\n",
    "            ).alias(\"messages\"),\n",
    "            F.lit(150).alias(\"max_completion_tokens\")  # Increased token limit\n",
    "        ).alias(\"body\")\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1732949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_batch.select(\"json_request\").coalesce(1).write.mode(\"overwrite\").json(\"output/batchinput_temp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbba9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cdd487a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batchinput.jsonl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Find the part file and rename to batchinput.jsonl\n",
    "part_file = glob.glob(\"output/batchinput_temp/part-*.json\")[0]\n",
    "shutil.move(part_file, \"batchinput.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602bb09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote fixed file: batchinput_fixed.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Fix double-encoded batchinput.jsonl to valid JSONL for OpenAI Batch API\n",
    "with open(\"batchinput.jsonl\", \"r\") as infile, open(\"batchinput_fixed.jsonl\", \"w\") as outfile:\n",
    "    for line in infile:\n",
    "        # Each line is a JSON object with a string value under 'json_request'\n",
    "        obj = json.loads(line)\n",
    "        # Parse the string value to a dict\n",
    "        fixed = json.loads(obj[\"json_request\"])\n",
    "        # Write as a single JSON object per line\n",
    "        outfile.write(json.dumps(fixed) + \"\\n\")\n",
    "\n",
    "print(\"Wrote fixed file: batchinput_fixed.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b542b9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 13 files in 'batches/' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_jsonl(input_path, lines_per_file=1000):\n",
    "    os.makedirs('batches', exist_ok=True)\n",
    "    with open(input_path, 'r') as infile:\n",
    "        file_count = 0\n",
    "        lines = []\n",
    "        for i, line in enumerate(infile, 1):\n",
    "            lines.append(line)\n",
    "            if i % lines_per_file == 0:\n",
    "                with open(f'batches/batch_{file_count}.jsonl', 'w') as out:\n",
    "                    out.writelines(lines)\n",
    "                lines = []\n",
    "                file_count += 1\n",
    "        if lines:\n",
    "            with open(f'batches/batch_{file_count}.jsonl', 'w') as out:\n",
    "                out.writelines(lines)\n",
    "    print(f\"Split into {file_count+1} files in 'batches/' directory.\")\n",
    "\n",
    "split_jsonl('batchinput_fixed.jsonl', lines_per_file=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import os\n",
    "client = OpenAI()\n",
    "batch_files = sorted(glob.glob(\"batches/batch_*.jsonl\"))\n",
    "os.makedirs(\"batch_results\", exist_ok=True)\n",
    "for batch_path in batch_files:\n",
    "    # 1. Upload batch file\n",
    "    upload = client.files.create(file=open(batch_path, \"rb\"), purpose=\"batch\")\n",
    "    print(f\"Uploaded {batch_path} as file ID: {upload.id}\")\n",
    "\n",
    "    # 2. Submit batch job\n",
    "    job = client.batches.create(\n",
    "        input_file_id=upload.id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": f\"Batch job for {batch_path}\"}\n",
    "    )\n",
    "    print(f\"Submitted batch job ID: {job.id}\")\n",
    "\n",
    "    # 3. Poll for job completion\n",
    "    while True:\n",
    "        status = client.batches.retrieve(job.id)\n",
    "        print(f\"Job {job.id} status: {status.status}\")\n",
    "        if status.status in [\"completed\", \"failed\", \"cancelled\"]:\n",
    "            break\n",
    "        time.sleep(10)  # Wait before checking again\n",
    "\n",
    "    # 4. Download results if completed\n",
    "    if status.status == \"completed\" and status.output_file_id:\n",
    "        print(f\"Batch {batch_path} completed. Downloading results...\")\n",
    "        result_file = client.files.retrieve(status.output_file_id)\n",
    "        content = client.files.content(status.output_file_id)\n",
    "        out_path = os.path.join(\"batch_results\", f\"results_{os.path.basename(batch_path)}\")\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(content.read())\n",
    "        print(f\"Results saved to {out_path}\")\n",
    "    else:\n",
    "        print(f\"Batch {batch_path} did not complete successfully. Check errors.\")\n",
    "\n",
    "    # Optional: Pause between batches to avoid rate limits\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb2c3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fresh Spark session created\n",
      "Recreating original dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 05:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/28 05:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 12000 rows\n",
      "Processing batch results...\n",
      "Processed 12000 results\n",
      "Performing join...\n",
      "Testing DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 05:51:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/28 05:51:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 12000\n",
      "\n",
      "Classification distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 05:52:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/28 05:52:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acceptable review: 7924\n",
      "Low quality review: 2278\n",
      "Rant without visit: 635\n",
      "5. Acceptable review: 254\n",
      "Label: Low quality review: 248\n",
      "Label: Acceptable review: 237\n",
      "Label: Rant without visit: 226\n",
      "Label: Irrelevant content: 75\n",
      "Irrelevant content: 75\n",
      "Label: Advertisement: 14\n",
      "Advertisement: 13\n",
      "4. Low quality review: 6\n",
      "3. Rant without visit: 4\n",
      "Classification: Low quality review: 3\n",
      "First review: \"Food was perfect, our server was very nice and made us laugh quite a bit. I definitely recommend this place.\" -> Acceptable review\n",
      "\n",
      "Second review: \"Came here yesterday as we had a great first time experience and wanted the same for my mother in law who was visiting, I arrived about 1pm and it wasn't busy. Server was very nice and attentive, I was though disappointed as my husband was with our steaks. He ordered his medium rare and mine medium, we couldn't finish them. My mashed potatoes had sour cream and I requested it not to be. I didnt not tell my server as I fear that bringing requesting it to be fixed with lead to hidden extra bodily contents. I dont claim this place does that: 1\n",
      "Label: 2. Irrelevant content: 1\n",
      "Classification: Rant without visit: 1\n",
      "Classify the following review into one of these labels:\n",
      "5. Acceptable review: 1\n",
      "Label: 3. Rant without visit: 1\n",
      "Classified as: Low quality review: 1\n",
      "Level: Acceptable review: 1\n",
      "Acceptance review: 1\n",
      "\n",
      "Saving results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 05:53:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/28 05:53:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet save successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/28 05:53:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/08/28 05:53:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CSV saved!\n",
      "Process completed!\n",
      "\n",
      "Sample results:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Acceptable review | Fabric store, Baking supply store, Bead store, Craft store, Knit shop, Picture frame shop, Scrapbooking store, Seasonal goods store, Sewing shop, Yarn store | Had what I needed....\n",
      "2. Acceptable review | American restaurant, Breakfast restaurant, Family restaurant, Hamburger restaurant | Good food n really good service...\n",
      "3. Acceptable review | Fast food restaurant, Breakfast restaurant, Chicken restaurant, Hamburger restaurant | The breakfast sandwich was cold!...\n",
      "4. Label: Low quality review | Mediterranean restaurant, Chicken shop, Coffee shop, Health food restaurant, Juice shop, Salad shop | They really care here you can tell as soon as you walk in. Foods wonderful give it a try!...\n",
      "5. Acceptable review | Hamburger restaurant, American restaurant, Fast food restaurant, Hot dog restaurant, Hot dog stand, Restaurant | Got to love Five Guys Burgers and Fries worth the wait every time...\n"
     ]
    }
   ],
   "source": [
    "# SIMPLIFIED VERSION - Just run this directly\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, concat_ws, rand, row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Stop existing session and create fresh one\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReviewClassificationSimple\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Fresh Spark session created\")\n",
    "\n",
    "# Recreate original data\n",
    "print(\"Recreating original dataset...\")\n",
    "pathing_review = \"datasets/review_data/\"\n",
    "arr = np.array(os.listdir(pathing_review))\n",
    "reviewData_files = pathing_review + arr\n",
    "\n",
    "pathing_metadata = \"datasets/review_metadata/\"\n",
    "arr = np.array(os.listdir(pathing_metadata))\n",
    "reviewMetadata_files = pathing_metadata + arr\n",
    "\n",
    "df_review = spark.read.json(list(reviewData_files)).dropna(subset=\"text\").drop_duplicates()\n",
    "df_metadata = spark.read.json(list(reviewMetadata_files)).dropna(subset=\"category\").drop_duplicates().select([\"gmap_id\", \"category\"])\n",
    "\n",
    "df_joined = df_review.join(df_metadata, on=\"gmap_id\", how=\"inner\") \\\n",
    "    .withColumn(\"category_str\", concat_ws(\", \", col(\"category\"))) \\\n",
    "    .withColumn(\"random_order\", rand()) \\\n",
    "    .orderBy(\"random_order\") \\\n",
    "    .drop(\"random_order\") \\\n",
    "    .limit(12000)\n",
    "\n",
    "# Add row numbers\n",
    "window = Window.orderBy(lit(1))\n",
    "df_with_row_num = df_joined.withColumn(\"row_number\", row_number().over(window) - 1)\n",
    "\n",
    "print(f\"Created DataFrame with {df_with_row_num.count()} rows\")\n",
    "\n",
    "# Process batch results\n",
    "print(\"Processing batch results...\")\n",
    "result_files = glob.glob(\"batch_results/results_batch_*.jsonl\")\n",
    "all_results = []\n",
    "\n",
    "for file_path in result_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            result = json.loads(line)\n",
    "            custom_id = result['custom_id']\n",
    "            row_num = int(custom_id.split('-')[1])\n",
    "            \n",
    "            if result.get('error') is None and result.get('response', {}).get('status_code') == 200:\n",
    "                classification = result['response']['body']['choices'][0]['message']['content'].strip()\n",
    "                all_results.append({\n",
    "                    'row_number': row_num,\n",
    "                    'classification': classification\n",
    "                })\n",
    "            else:\n",
    "                all_results.append({\n",
    "                    'row_number': row_num,\n",
    "                    'classification': 'Error'\n",
    "                })\n",
    "\n",
    "all_results.sort(key=lambda x: x['row_number'])\n",
    "print(f\"Processed {len(all_results)} results\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_schema = StructType([\n",
    "    StructField(\"row_number\", IntegerType(), True),\n",
    "    StructField(\"classification\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_results = spark.createDataFrame(all_results, schema=results_schema)\n",
    "\n",
    "# Perform join\n",
    "print(\"Performing join...\")\n",
    "df_final = df_with_row_num.join(df_results, on=\"row_number\", how=\"left\").drop(\"row_number\")\n",
    "\n",
    "# Test basic operations\n",
    "print(\"Testing DataFrame...\")\n",
    "try:\n",
    "    total_count = df_final.count()\n",
    "    print(f\"Total rows: {total_count}\")\n",
    "    \n",
    "    print(\"\\nClassification distribution:\")\n",
    "    classification_results = df_final.groupBy(\"classification\").count().collect()\n",
    "    \n",
    "    # Sort and display results manually to avoid show() issues\n",
    "    sorted_results = sorted(classification_results, key=lambda x: x['count'], reverse=True)\n",
    "    for row in sorted_results:\n",
    "        print(f\"{row['classification']}: {row['count']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during operations: {e}\")\n",
    "    # Let's try a simpler approach - just save the data\n",
    "    print(\"Attempting direct save...\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "try:\n",
    "    # Try saving with fewer partitions\n",
    "    df_final.coalesce(1).write.mode(\"overwrite\").parquet(\"output/classified_reviews\")\n",
    "    print(\"Parquet save successful!\")\n",
    "    \n",
    "    # Also save a sample as CSV for easy viewing\n",
    "    df_final.select(\"text\", \"category_str\", \"rating\", \"classification\") \\\n",
    "           .limit(1000) \\\n",
    "           .coalesce(1) \\\n",
    "           .write.mode(\"overwrite\") \\\n",
    "           .option(\"header\", \"true\") \\\n",
    "           .csv(\"output/sample_classified_reviews\")\n",
    "    print(\"Sample CSV saved!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Save error: {e}\")\n",
    "    # Try JSON format instead\n",
    "    try:\n",
    "        df_final.coalesce(1).write.mode(\"overwrite\").json(\"output/classified_reviews_json\")\n",
    "        print(\"JSON save successful!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"JSON save also failed: {e2}\")\n",
    "\n",
    "print(\"Process completed!\")\n",
    "\n",
    "# Optional: Show a few sample rows if possible\n",
    "try:\n",
    "    print(\"\\nSample results:\")\n",
    "    sample_data = df_final.select(\"text\", \"category_str\", \"classification\").limit(5).collect()\n",
    "    for i, row in enumerate(sample_data):\n",
    "        print(f\"{i+1}. {row['classification']} | {row['category_str']} | {row['text'][:100]}...\")\n",
    "except:\n",
    "    print(\"Could not display sample results, but data should be saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d18f5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f1766c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_classified_reviews = pd.read_parquet(\"output/classified_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1200f62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category_str</th>\n",
       "      <th>rating</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great people. A one stop store for anything yo...</td>\n",
       "      <td>Grocery store, Propane supplier</td>\n",
       "      <td>5</td>\n",
       "      <td>Acceptable review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Always spend too much!</td>\n",
       "      <td>Warehouse store, Department store</td>\n",
       "      <td>5</td>\n",
       "      <td>Acceptable review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bring your appetite. Best tortas in town.</td>\n",
       "      <td>Mexican restaurant, Breakfast restaurant, Burr...</td>\n",
       "      <td>5</td>\n",
       "      <td>Low quality review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I like shopping here</td>\n",
       "      <td>Sporting goods store, Boat dealer, Camping sto...</td>\n",
       "      <td>5</td>\n",
       "      <td>Acceptable review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fine if you want to browse for video games. Th...</td>\n",
       "      <td>Video game store, Cell phone store, Collectibl...</td>\n",
       "      <td>3</td>\n",
       "      <td>Acceptable review</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Great people. A one stop store for anything yo...   \n",
       "1                             Always spend too much!   \n",
       "2          Bring your appetite. Best tortas in town.   \n",
       "3                               I like shopping here   \n",
       "4  Fine if you want to browse for video games. Th...   \n",
       "\n",
       "                                        category_str  rating  \\\n",
       "0                    Grocery store, Propane supplier       5   \n",
       "1                  Warehouse store, Department store       5   \n",
       "2  Mexican restaurant, Breakfast restaurant, Burr...       5   \n",
       "3  Sporting goods store, Boat dealer, Camping sto...       5   \n",
       "4  Video game store, Cell phone store, Collectibl...       3   \n",
       "\n",
       "       classification  \n",
       "0   Acceptable review  \n",
       "1   Acceptable review  \n",
       "2  Low quality review  \n",
       "3   Acceptable review  \n",
       "4   Acceptable review  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified_reviews[[\"text\", \"category_str\", \"rating\", \"classification\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe83a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_classification_labels(df, classification_col='classification'):\n",
    "    \"\"\"\n",
    "    Clean inconsistent classification labels to standard format\n",
    "    \"\"\"\n",
    "    # Define the target labels\n",
    "    target_labels = [\n",
    "        'Advertisement',\n",
    "        'Irrelevant content', \n",
    "        'Rant without visit',\n",
    "        'Low quality review',\n",
    "        'Acceptable review'\n",
    "    ]\n",
    "    \n",
    "    def standardize_label(label):\n",
    "        if pd.isna(label) or label == 'Error':\n",
    "            return 'Error'\n",
    "        \n",
    "        # Convert to string and clean\n",
    "        label = str(label).strip()\n",
    "        \n",
    "        # Remove common prefixes\n",
    "        prefixes_to_remove = [\n",
    "            r'^Label:\\s*',\n",
    "            r'^Classification:\\s*',\n",
    "            r'^Classified as:\\s*',\n",
    "            r'^Level:\\s*',\n",
    "            r'^\\d+\\.\\s*',  # Remove numbers like \"5. \"\n",
    "            r'^Classify.*?labels?:\\s*',\n",
    "        ]\n",
    "        \n",
    "        for prefix in prefixes_to_remove:\n",
    "            label = re.sub(prefix, '', label, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Handle specific variations\n",
    "        label_lower = label.lower().strip()\n",
    "        \n",
    "        if 'advertisement' in label_lower or 'promotional' in label_lower:\n",
    "            return 'Advertisement'\n",
    "        elif 'irrelevant' in label_lower:\n",
    "            return 'Irrelevant content'\n",
    "        elif 'rant' in label_lower and 'visit' in label_lower:\n",
    "            return 'Rant without visit'\n",
    "        elif 'low quality' in label_lower:\n",
    "            return 'Low quality review'\n",
    "        elif 'acceptable' in label_lower or 'acceptance' in label_lower:\n",
    "            return 'Acceptable review'\n",
    "        else:\n",
    "            # Try exact match after cleaning\n",
    "            for target in target_labels:\n",
    "                if target.lower() == label_lower:\n",
    "                    return target\n",
    "            \n",
    "            # If no match found, return original for manual inspection\n",
    "            return f\"UNMATCHED: {label}\"\n",
    "    \n",
    "    # Apply cleaning\n",
    "    df_clean = df.copy()\n",
    "    df_clean[f'{classification_col}_cleaned'] = df_clean[classification_col].apply(standardize_label)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Clean the labels\n",
    "df_cleaned = clean_classification_labels(df_classified_reviews)\n",
    "\n",
    "# Show before and after comparison\n",
    "print(\"BEFORE CLEANING:\")\n",
    "print(df_classified_reviews['classification'].value_counts())\n",
    "\n",
    "print(\"\\nAFTER CLEANING:\")\n",
    "print(df_cleaned['classification_cleaned'].value_counts())\n",
    "\n",
    "# Check unmatched labels\n",
    "unmatched = df_cleaned[df_cleaned['classification_cleaned'].str.startswith('UNMATCHED')]\n",
    "if len(unmatched) > 0:\n",
    "    print(f\"\\nUNMATCHED LABELS ({len(unmatched)} rows):\")\n",
    "    print(unmatched['classification_cleaned'].value_counts())\n",
    "\n",
    "# Replace original column with cleaned version\n",
    "df_classified_reviews['classification'] = df_cleaned['classification_cleaned']\n",
    "\n",
    "# Final clean counts\n",
    "print(\"\\nFINAL CLEANED DISTRIBUTION:\")\n",
    "final_counts = df_classified_reviews['classification'].value_counts()\n",
    "print(final_counts)\n",
    "\n",
    "# Calculate percentages\n",
    "print(\"\\nPERCENTAGES:\")\n",
    "percentages = (final_counts / len(df_classified_reviews) * 100).round(1)\n",
    "for label, count in final_counts.items():\n",
    "    pct = percentages[label]\n",
    "    print(f\"{label}: {count} ({pct}%)\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df_classified_reviews.to_csv('output/classified_reviews_cleaned.csv', index=False)\n",
    "print(f\"\\nCleaned dataset saved to 'output/classified_reviews_cleaned.csv'\")\n",
    "print(f\"Total rows: {len(df_classified_reviews)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6709be58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification\n",
       "Acceptable review     8419\n",
       "Low quality review    2536\n",
       "Rant without visit     867\n",
       "Irrelevant content     151\n",
       "Advertisement           27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classified_reviews['classification'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ee4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test a single request to OpenAI API with 'gpt-5-nano' to verify model access and request validity\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-5-nano\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": (\n",
    "#                 \"Classify the following review into one of these labels: \"\n",
    "#                 \"1. Advertisement (promotional content or links) \"\n",
    "#                 \"2. Irrelevant content (not about the location) \"\n",
    "#                 \"3. Rant without visit (complaints from non-visitors) \"\n",
    "#                 \"4. Low quality review (little info or random emojis) \"\n",
    "#                 \"5. Acceptable review (genuine experience at the location)\\n\\n\"\n",
    "#                 \"Examples:\\n\"\n",
    "#                 \"\\\"Best pizza! Visit www.pizzapromo.com for discounts!\\\" → Advertisement\\n\"\n",
    "#                 \"\\\"I love my new phone, but this place is too noisy.\\\" → Irrelevant content\\n\"\n",
    "#                 \"\\\"Never been here, but I heard it is terrible.\\\" → Rant without visit\\n\"\n",
    "#                 \"\\\"Good\\\" → Low quality review\\n\"\n",
    "#                 \"\\\"I have gotten two piercings done by Adner. He is very clean, calm, and knowledgeable...\\\" → Acceptable review\\n\\n\"\n",
    "#                 \"Helpful tags to help decide what classification types to assign: Dermatologist, Cancer treatment center, Medical Center, Skin care clinic\\n\\n\"\n",
    "#             )\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Review: As always, it was easy to make an appointment and I was seen on time . The staff was great and made me feel like a valued customer! I highly recommend this Office\"\n",
    "#         }\n",
    "#     ],\n",
    "#     max_completion_tokens=1024\n",
    "# )\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
