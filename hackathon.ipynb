{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590f22ef",
   "metadata": {},
   "source": [
    "# Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e9c6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, concat_ws, rand, lit\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e29fa5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:58:36 WARN Utils: Your hostname, Asyrafs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.18.78 instead (on interface en0)\n",
      "25/08/30 22:58:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/trigger/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/trigger/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e85da8c3-7ddc-42a5-be27-b5de6aa2782d;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/trigger/Documents/GitHub/tiktok-hackathon/.venv_hackathon/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-s3;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-kms;1.12.500 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-core;1.12.500 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound software.amazon.ion#ion-java;1.0.2 in central\n",
      "\tfound joda-time#joda-time;2.8.1 in central\n",
      "\tfound com.amazonaws#jmespath-java;1.12.500 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.20.1 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.18.0 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in local-m2-cache\n",
      "\tfound com.google.http-client#google-http-client;1.43.0 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.43.0 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.43.0 in central\n",
      "\tfound com.google.api-client#google-api-client;2.2.0 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.43.0 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10.1 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.12.0 in central\n",
      "\tfound io.grpc#grpc-context;1.53.0 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n",
      "\tfound com.google.auto.value#auto-value;1.10.1 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.12.0 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.43.0 in central\n",
      "\tfound com.google.api#gax-httpjson;0.108.2 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.12.0 in central\n",
      "\tfound io.grpc#grpc-alts;1.53.0 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.53.0 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf;1.53.0 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.53.0 in central\n",
      "\tfound io.grpc#grpc-core;1.53.0 in central\n",
      "\tfound com.google.api#gax;2.23.2 in central\n",
      "\tfound com.google.api#gax-grpc;2.23.2 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.16.0 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.16.0 in central\n",
      "\tfound com.google.api#api-common;2.6.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.9.2 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.12 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.12 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.14.2 in central\n",
      "\tfound org.threeten#threetenbp;1.6.5 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound io.grpc#grpc-api;1.53.0 in central\n",
      "\tfound io.grpc#grpc-stub;1.53.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      "\tfound io.perfmark#perfmark-api;0.26.0 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.53.0 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.53.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.53.0 in central\n",
      "\tfound io.grpc#grpc-xds;1.53.0 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      "\tfound com.microsoft.onnxruntime#onnxruntime;1.19.2 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 in central\n",
      "\tfound org.jetbrains#annotations;24.1.0 in central\n",
      "\tfound com.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 in central\n",
      ":: resolution report :: resolve 580ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-core;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-kms;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-s3;1.12.500 from central in [default]\n",
      "\tcom.amazonaws#jmespath-java;1.12.500 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.6.2 from central in [default]\n",
      "\tcom.google.api#gax;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.23.2 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.108.2 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.2.0 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.20.1-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.14.2 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.9.2 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.16.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.16.0 from central in [default]\n",
      "\tcom.google.auto.value#auto-value;1.10.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.12.0 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.20.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.10.1 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.18.0 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.http-client#google-http-client;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.43.0 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.43.0 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from local-m2-cache in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.12 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.12 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-llamacpp-cpu_2.12;0.1.1-rc2 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#jsl-openvino-cpu_2.12;0.1.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;5.5.0 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.microsoft.onnxruntime#onnxruntime;1.19.2 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-api;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-context;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-core;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-services;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.53.0 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.53.0 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\tjoda-time#joda-time;2.8.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.jetbrains#annotations;24.1.0 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.5 from central in [default]\n",
      "\tsoftware.amazon.ion#ion-java;1.0.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcommons-logging#commons-logging;1.2 by [commons-logging#commons-logging;1.1.3] in [default]\n",
      "\tcommons-codec#commons-codec;1.11 by [commons-codec#commons-codec;1.15] in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.12] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.12] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   86  |   0   |   0   |   5   ||   81  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e85da8c3-7ddc-42a5-be27-b5de6aa2782d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 81 already retrieved (0kB/8ms)\n",
      "25/08/30 22:58:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Hackathon\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fb8052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pathing_review = \"datasets/review_data/\"\n",
    "arr = np.array(os.listdir(pathing_review))\n",
    "reviewData_files = pathing_review + arr\n",
    "\n",
    "pathing_metadata = \"datasets/review_metadata/\"\n",
    "arr = np.array(os.listdir(pathing_metadata))\n",
    "reviewMetadata_files = pathing_metadata + arr\n",
    "\n",
    "df_review = spark.read.json(list(reviewData_files)).dropna(subset=\"text\").drop_duplicates()\n",
    "df_metadata = spark.read.json(list(reviewMetadata_files)).dropna(subset=\"category\").drop_duplicates().withColumnRenamed(\"name\", \"business_name\").select([\"gmap_id\", \"category\", \"business_name\"])\n",
    "\n",
    "df_joined = df_review.join(df_metadata, on=\"gmap_id\", how=\"inner\").withColumn(\"category_str\", concat_ws(\", \", col(\"category\"))).withColumn(\"random_order\", rand()).orderBy(\"random_order\").drop(\"random_order\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab35bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- business_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_metadata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc3a07c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pics: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- resp: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- time: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gmap_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- pics: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- url: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- rating: long (nullable = true)\n",
      " |-- resp: struct (nullable = true)\n",
      " |    |-- text: string (nullable = true)\n",
      " |    |-- time: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- category: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- business_name: string (nullable = true)\n",
      " |-- category_str: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_joined.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec83cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count, size, trim\n",
    "from pyspark.sql.types import StringType, FloatType, DoubleType, ArrayType, MapType\n",
    "\n",
    "# 1) Drop unwanted columns\n",
    "df_filtered = df_joined.drop(\"pics\", \"resp\", \"time\", \"category\", \"user_id\")\n",
    "\n",
    "# 2) Build a \"is missing\" condition per column based on its data type\n",
    "missing_conds = []\n",
    "for f in df_filtered.schema.fields:\n",
    "    c = col(f.name)\n",
    "    dt = f.dataType\n",
    "\n",
    "    if isinstance(dt, (FloatType, DoubleType)):\n",
    "        # floats/doubles: NULL or NaN\n",
    "        cond = c.isNull() | c.isnan()\n",
    "    elif isinstance(dt, StringType):\n",
    "        # strings: NULL or empty after trim\n",
    "        cond = c.isNull() | (trim(c) == \"\")\n",
    "    elif isinstance(dt, (ArrayType, MapType)):\n",
    "        # arrays/maps: NULL or empty\n",
    "        cond = c.isNull() | (size(c) == 0)\n",
    "    else:\n",
    "        # ints/longs/booleans/date/timestamp/structs: only NULL\n",
    "        cond = c.isNull()\n",
    "\n",
    "    missing_conds.append(count(when(cond, True)).alias(f.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cc7dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====================================>                    (9 + 5) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+----+-------------+------------+\n",
      "|gmap_id|name|rating|text|business_name|category_str|\n",
      "+-------+----+------+----+-------------+------------+\n",
      "|0      |0   |0     |0   |0            |0           |\n",
      "+-------+----+------+----+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "missing_counts = df_filtered.select(missing_conds)\n",
    "missing_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b014d8d",
   "metadata": {},
   "source": [
    "### Some reviews have translation, we only need English ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e1e7db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==========================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                       text|\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|                                               (Translated by Google) Nice place\\n\\n(Original)\\nLindo lugar|\n",
      "|       (Translated by Google) Visit with respect and respect ..., very nice experience!\\n\\n(Original)\\nM...|\n",
      "|       (Translated by Google) It is a nice place the only detail is that it does not have bathrooms and ...|\n",
      "|(Translated by Google) It's delicious and very good.\\nPromise to go back\\n\\n(Original)\\në§›ìžˆê³  ì•„ì£¼ ì¢‹ì•„...|\n",
      "|       (Translated by Google) Very friendly staff and all their fresh products\\n\\n(Original)\\nEl persona...|\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.filter(df_joined.text.contains(\"(Translated by Google)\")).select(\"text\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1209433",
   "metadata": {},
   "source": [
    "### Some reviews have many spacings, remove spacings for one-line reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e636a433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=========================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                text|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|          Been doing there a long time. Always get a great haircut -\\nMichelle's the Best!! ðŸ‘ ðŸ‘ ðŸ‘|\n",
      "|Very impressed! Dawn and Jazz are AMAZING!\\nMake your appointments now for all of your holiday ga...|\n",
      "|Awesome customer service. We visited this store for backsplash tiles and one of their design cons...|\n",
      "|[WARNING] outstanding food, highly addictive. The only problem is it attracts a massive amount of...|\n",
      "|BEST BBQ IN PHOENIX!!!\\n\\nStopped Today and it didnâ€™t disappoint!!! It was soo good!!! I got the ...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_joined.filter(F.col(\"text\").rlike(\"\\n\")).select(\"text\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee00a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing (Translated by Google) prefix and (Original) languages to get the English reviews \n",
    "# Removing all newlines for one-lined reviews\n",
    "# Removing quotation marks\n",
    "\n",
    "df_joined = df_joined.withColumn(\n",
    "    \"text\",\n",
    "    F.when(\n",
    "        F.col(\"text\").contains(\"(Translated by Google)\"),\n",
    "        # extract the English text, remove newlines, remove quotes\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(\n",
    "                F.regexp_extract(F.col(\"text\"), r\"\\(Translated by Google\\)\\s*([^\\n]+)\", 1),\n",
    "                r\"\\n+\", \" \"\n",
    "            ),\n",
    "            r\"\\\"\", \"\"\n",
    "        )\n",
    "    ).otherwise(\n",
    "        # for rows without Google Translate tag, remove newlines and quotes\n",
    "        F.regexp_replace(\n",
    "            F.regexp_replace(F.col(\"text\"), r\"\\n+\", \" \"),\n",
    "            r\"\\\"\", \"\"\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536c1b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:58:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark_nlp = sparknlp.start(apple_silicon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b56c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:02 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:07 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_norvig download started this may take some time.\n",
      "Approximate size to download 4.2 MB\n",
      "Download done! Loading the resource.\n",
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:15 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 907.6 KB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:16 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/08/30 22:59:17 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:22 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 145.3 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:22 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/08/30 22:59:23 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_100d download started this may take some time.\n",
      "Approximate size to download 145.3 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Document Assembler\n",
    "# -------------------------------\n",
    "customer_review = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"customer_review\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Tokenizer\n",
    "# -------------------------------\n",
    "customer_review_token = Tokenizer() \\\n",
    "    .setInputCols([\"customer_review\"]) \\\n",
    "    .setOutputCol(\"customer_review_token\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Spell Checker\n",
    "# -------------------------------\n",
    "customer_review_spell_checker = NorvigSweetingModel.pretrained() \\\n",
    "    .setInputCols([\"customer_review_token\"]) \\\n",
    "    .setOutputCol(\"customer_review_corrected\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Normalizer (lowercasing, clean text)\n",
    "# -------------------------------\n",
    "customer_review_normalizer = Normalizer() \\\n",
    "    .setInputCols([\"customer_review_corrected\"]) \\\n",
    "    .setOutputCol(\"customer_review_normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. StopWords Cleaner\n",
    "# -------------------------------\n",
    "customer_review_stopwordsCleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"customer_review_normalized\"]) \\\n",
    "    .setOutputCol(\"customer_review_cleaned\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Lemmatizer\n",
    "# -------------------------------\n",
    "customer_review_lemma = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"customer_review_token\"]) \\\n",
    "    .setOutputCol(\"customer_review_lemma\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Word Embeddings (GloVe)\n",
    "# -------------------------------\n",
    "glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
    "    .setInputCols([\"customer_review_token\", \"customer_review\"]) \\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Sentence Embeddings (average pooling)\n",
    "# -------------------------------\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"customer_review\", \"word_embeddings\"]) \\\n",
    "    .setOutputCol(\"customer_review_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Embeddings Finisher (convert to Spark vector/array)\n",
    "# -------------------------------\n",
    "customer_review_finisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"customer_review_embeddings\"]) \\\n",
    "    .setOutputCols([\"customer_review_vector\"]) \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7d2008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_norvig download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:27 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 4.2 MB\n",
      "[OK!]\n",
      "lemma_antbnc download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:31 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "glove_100d download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:34 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 145.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Document Assembler\n",
    "# -------------------------------\n",
    "business_category = DocumentAssembler() \\\n",
    "    .setInputCol(\"category_str\") \\\n",
    "    .setOutputCol(\"business_category\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Tokenizer\n",
    "# -------------------------------\n",
    "business_category_token = Tokenizer() \\\n",
    "    .setInputCols([\"business_category\"]) \\\n",
    "    .setOutputCol(\"business_category_token\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Spell Checker\n",
    "# -------------------------------\n",
    "business_category_spell_checker = NorvigSweetingModel.pretrained() \\\n",
    "    .setInputCols([\"business_category_token\"]) \\\n",
    "    .setOutputCol(\"business_category_corrected\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Normalizer (lowercasing, clean text)\n",
    "# -------------------------------\n",
    "business_category_normalizer = Normalizer() \\\n",
    "    .setInputCols([\"business_category_corrected\"]) \\\n",
    "    .setOutputCol(\"business_category_normalized\") \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. StopWords Cleaner\n",
    "# -------------------------------\n",
    "business_category_stopwordsCleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"business_category_normalized\"]) \\\n",
    "    .setOutputCol(\"business_category_cleaned\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Lemmatizer\n",
    "# -------------------------------\n",
    "business_category_lemma = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols([\"business_category_token\"]) \\\n",
    "    .setOutputCol(\"business_category_lemma\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Word Embeddings (GloVe)\n",
    "# -------------------------------\n",
    "business_category_glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
    "    .setInputCols([\"business_category_token\", \"business_category\"]) \\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Sentence Embeddings (average pooling)\n",
    "# -------------------------------\n",
    "business_category_sentence_embeddings = SentenceEmbeddings() \\\n",
    "    .setInputCols([\"business_category\", \"word_embeddings\"]) \\\n",
    "    .setOutputCol(\"business_category_embeddings\") \\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Embeddings Finisher (convert to Spark vector/array)\n",
    "# -------------------------------\n",
    "business_category_finisher = EmbeddingsFinisher() \\\n",
    "    .setInputCols([\"business_category_embeddings\"]) \\\n",
    "    .setOutputCols([\"business_category_vector\"]) \\\n",
    "    .setOutputAsVector(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109b9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both into one pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    # --- Review branch ---\n",
    "    customer_review,\n",
    "    customer_review_token,\n",
    "    customer_review_spell_checker,\n",
    "    customer_review_normalizer,\n",
    "    customer_review_stopwordsCleaner,\n",
    "    customer_review_lemma,\n",
    "    glove_embeddings,\n",
    "    sentence_embeddings,\n",
    "    customer_review_finisher,\n",
    "\n",
    "    # --- Business category branch ---\n",
    "    business_category,\n",
    "    business_category_token,\n",
    "    business_category_spell_checker,\n",
    "    business_category_normalizer,\n",
    "    business_category_stopwordsCleaner,\n",
    "    business_category_lemma,\n",
    "    business_category_glove_embeddings,\n",
    "    business_category_sentence_embeddings,\n",
    "    business_category_finisher\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c74523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/Users/trigger/Documents/GitHub/tiktok-hackathon/.venv_hackathon/lib/python3.11/site-packages/pyspark/jars/spark-core_2.12-3.5.6.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    }
   ],
   "source": [
    "result = pipeline.fit(df_joined).transform(df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(\"text\", \"customer_review_vector\").show(5, truncate=50, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5802c6",
   "metadata": {},
   "source": [
    "### Cosine similarity between review and business category for relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b56449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    if v1 is None or v2 is None:\n",
    "        return None\n",
    "    a = np.asarray(v1, dtype=float)\n",
    "    b = np.asarray(v2, dtype=float)\n",
    "\n",
    "    # squeeze 1xN / Nx1 or nested singletons to 1-D\n",
    "    if a.ndim > 1:\n",
    "        a = a.reshape(-1)\n",
    "    if b.ndim > 1:\n",
    "        b = b.reshape(-1)\n",
    "\n",
    "    na = np.linalg.norm(a)\n",
    "    nb = np.linalg.norm(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "cosine_sim_udf = udf(cosine_similarity, DoubleType())\n",
    "\n",
    "result = result.withColumn(\n",
    "    \"cosine_similarity\",\n",
    "    cosine_sim_udf(\"customer_review_vector\", \"business_category_vector\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select(\"category_str\", \"text\", \"cosine_similarity\").show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1119a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "base = (result\n",
    "    .select(\"business_name\",\"category_str\",\"text\",\"rating\",\"cosine_similarity\")\n",
    "    .filter(F.col(\"cosine_similarity\").isNotNull())\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = result.select(\"cosine_similarity\").filter(F.col(\"cosine_similarity\").isNotNull())\n",
    "\n",
    "# Global quantiles\n",
    "q = test.approxQuantile(\"cosine_similarity\", [0.25, 0.5, 0.75], 0.01)\n",
    "LO, MED, HI = q[0], q[1], q[2]\n",
    "print(\"LO, MED, HI =\", LO, MED, HI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "810225ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_once(df, level=StorageLevel.MEMORY_AND_DISK):\n",
    "    # only cache if this instance isnâ€™t already cached\n",
    "    if not df.is_cached:\n",
    "        df = df.persist(level)\n",
    "    return df\n",
    "\n",
    "benchmark = 0.70\n",
    "low = 0.60\n",
    "\n",
    "relevant_df  = cache_once(base.filter(F.col(\"cosine_similarity\") >= benchmark))\n",
    "irrelevant_df = cache_once(base.filter(F.col(\"cosine_similarity\") <= low))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e854f47",
   "metadata": {},
   "source": [
    "### Operations to find promotional links or advertisments in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de023fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper funtions\n",
    "\n",
    "def add_ad_key(df):\n",
    "    cols = set(df.columns)\n",
    "\n",
    "    def safe(name, cast_str=False):\n",
    "        if name in cols:\n",
    "            c = F.col(name)\n",
    "            if cast_str:\n",
    "                c = c.cast(\"string\")\n",
    "            return F.coalesce(c, F.lit(\"\"))\n",
    "        else:\n",
    "            return F.lit(\"\")\n",
    "\n",
    "    # Build a stable key from available fields (order matters).\n",
    "    # Include text + business_name + category_str + rating + cosine_similarity; add ids/time if present.\n",
    "    return df.withColumn(\n",
    "        \"ad_key\",\n",
    "        F.sha2(F.concat_ws(\"||\",\n",
    "            safe(\"gmap_id\"),\n",
    "            safe(\"user_id\"),\n",
    "            safe(\"business_name\"),\n",
    "            safe(\"category_str\"),\n",
    "            safe(\"time\", cast_str=True),\n",
    "            safe(\"text\"),\n",
    "            safe(\"rating\", cast_str=True),\n",
    "            safe(\"cosine_similarity\", cast_str=True)\n",
    "        ), 256)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def with_ads_flags(df):\n",
    "    txt = F.coalesce(F.col(\"text\"), F.lit(\"\"))\n",
    "\n",
    "    return (df\n",
    "      # URLs / domains / shorteners / obfuscations\n",
    "      .withColumn(\"has_url\",          txt.rlike(r\"(?i)\\bhttps?://\\S+|\\bwww\\.\\S+\"))\n",
    "      .withColumn(\"has_domain\",       txt.rlike(r\"(?i)\\b[a-z0-9][a-z0-9\\-]*\\.(?:com|net|org|co|io|info|biz|app|shop|store|sg|uk|au|ca|de|fr|my|ph|id|in)(?:/\\S*)?\\b\"))\n",
    "      .withColumn(\"has_shortener\",    txt.rlike(r\"(?i)\\b(bit\\.ly|t\\.co|goo\\.gl|tinyurl\\.com|ow\\.ly|wa\\.me|linktr\\.ee)/\\S+\"))\n",
    "      .withColumn(\"has_obfus_domain\", txt.rlike(r\"(?i)\\b[a-z0-9][a-z0-9\\-]*\\s*(?:\\.|dot|\\[\\.]|\\(dot\\))\\s*(?:com|net|org|co|io|sg|au|uk)\\b\"))\n",
    "\n",
    "      # Contact info / WhatsApp\n",
    "      .withColumn(\"has_email\",        txt.rlike(r\"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\"))\n",
    "      .withColumn(\"has_phone\",        txt.rlike(r\"(?i)(?:\\+?\\d[\\s\\-().]{0,3}){7,}\\d\"))\n",
    "      .withColumn(\"has_whatsapp\",     txt.rlike(r\"(?i)\\bwhatsapp\\b|\\bwa\\.me/\\S+\"))\n",
    "\n",
    "      # Promo / CTA phrases\n",
    "      .withColumn(\"has_promo_words\",  txt.rlike(r\"(?i)\\b(promo(?:\\s*code)?|discount(?:\\s*code)?|coupon|use\\s+code|deal|sale|flash\\s*sale|limited\\s*time|special\\s*offer|[0-9]{1,3}%\\s*off|buy\\s*now|order\\s*now|book\\s*now|free\\s*shipping|visit\\s+(?:our\\s+)?website|click\\s+(?:here|link))\\b\"))\n",
    "\n",
    "      # Final flag + triggers for auditability\n",
    "      .withColumn(\"policy_ads\",\n",
    "          F.col(\"has_url\") | F.col(\"has_domain\") | F.col(\"has_shortener\") |\n",
    "          F.col(\"has_obfus_domain\") | F.col(\"has_email\") | F.col(\"has_phone\") |\n",
    "          F.col(\"has_whatsapp\") | F.col(\"has_promo_words\")\n",
    "      )\n",
    "      .withColumn(\"ads_triggers\", F.array_remove(F.array(\n",
    "          F.when(F.col(\"has_url\"),          F.lit(\"url\")),\n",
    "          F.when(F.col(\"has_domain\"),       F.lit(\"domain\")),\n",
    "          F.when(F.col(\"has_shortener\"),    F.lit(\"shortener\")),\n",
    "          F.when(F.col(\"has_obfus_domain\"), F.lit(\"obfus_domain\")),\n",
    "          F.when(F.col(\"has_email\"),        F.lit(\"email\")),\n",
    "          F.when(F.col(\"has_phone\"),        F.lit(\"phone\")),\n",
    "          F.when(F.col(\"has_whatsapp\"),     F.lit(\"whatsapp\")),\n",
    "          F.when(F.col(\"has_promo_words\"),  F.lit(\"promo_words\"))\n",
    "      ), None))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c814c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_flagged   = add_ad_key(with_ads_flags(relevant_df))\n",
    "irrelevant_flagged = add_ad_key(with_ads_flags(irrelevant_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a782a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_union = (relevant_flagged.withColumn(\"source_split\", F.lit(\"relevant\"))\n",
    "             .unionByName(irrelevant_flagged.withColumn(\"source_split\", F.lit(\"irrelevant\"))))\n",
    "\n",
    "ads_only = (ads_union\n",
    "            .filter(F.col(\"policy_ads\"))\n",
    "            .dropDuplicates([\"ad_key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b195e227",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads_keys = ads_only.select(\"ad_key\")\n",
    "\n",
    "relevant_clean = (relevant_flagged.join(ads_keys, on=\"ad_key\", how=\"left_anti\")\n",
    "                  .drop(\"has_url\",\"has_domain\",\"has_shortener\",\"has_obfus_domain\",\n",
    "                        \"has_email\",\"has_phone\",\"has_whatsapp\",\"has_promo_words\",\n",
    "                        \"policy_ads\",\"ads_triggers\"))\n",
    "\n",
    "irrelevant_clean = (irrelevant_flagged.join(ads_keys, on=\"ad_key\", how=\"left_anti\")\n",
    "                    .drop(\"has_url\",\"has_domain\",\"has_shortener\",\"has_obfus_domain\",\n",
    "                          \"has_email\",\"has_phone\",\"has_whatsapp\",\"has_promo_words\",\n",
    "                          \"policy_ads\",\"ads_triggers\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859d769",
   "metadata": {},
   "source": [
    "### Find reviews that are rants without visit in irrelevant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51fc63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the base irrelevant dataframe safely\n",
    "try:\n",
    "    irr_base = irrelevant_clean\n",
    "except NameError:\n",
    "    try:\n",
    "        irr_base = irrelevant_df\n",
    "    except NameError:\n",
    "        irr_base = irrelevant_df  \n",
    "\n",
    "irr_base = irr_base.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18ccfee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:40 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 873.6 KB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:41 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/08/30 22:59:42 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment_vivekn download started this may take some time.\n",
      "Approximate size to download 873.6 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "def ensure_sentiment(df):\n",
    "    if \"sentiment_num\" in df.columns:\n",
    "        return df\n",
    "\n",
    "    # Prefer corrected tokens if available\n",
    "    use_corr = \"customer_review_corrected\" in df.columns\n",
    "    text_col = \"text\"\n",
    "\n",
    "    if use_corr:\n",
    "        from sparknlp.base import Finisher\n",
    "        df = (Finisher()\n",
    "              .setInputCols([\"customer_review_corrected\"])\n",
    "              .setOutputCols([\"corr_tokens\"])\n",
    "              .setOutputAsArray(True)\n",
    "              .setCleanAnnotations(True)\n",
    "             ).transform(df)\n",
    "        df = df.withColumn(\"text_corrected\", F.array_join(\"corr_tokens\", \" \"))\n",
    "        text_col = \"text_corrected\"\n",
    "\n",
    "   \n",
    "\n",
    "    document = DocumentAssembler().setInputCol(text_col).setOutputCol(\"doc\")\n",
    "    token    = Tokenizer().setInputCols([\"doc\"]).setOutputCol(\"tok\")\n",
    "    viv      = ViveknSentimentModel.pretrained().setInputCols([\"doc\",\"tok\"]).setOutputCol(\"sent\")\n",
    "    pipe     = Pipeline(stages=[document, token, viv]).fit(df)\n",
    "\n",
    "    out = pipe.transform(df)\n",
    "    out = (out\n",
    "        .withColumn(\"sentiment_str\", F.expr(\"sent[0].result\"))\n",
    "        .withColumn(\"prob_pos\", F.expr(\"cast(sent[0].metadata['positive'] as double)\"))\n",
    "        .withColumn(\"prob_neg\", F.expr(\"cast(sent[0].metadata['negative'] as double)\"))\n",
    "        .drop(\"sent\")\n",
    "    )\n",
    "    # margin to avoid shaky labels\n",
    "    out = out.withColumn(\n",
    "        \"sentiment_num\",\n",
    "        F.when(F.col(\"prob_pos\") - F.col(\"prob_neg\") >= 0.2, 1.0)\n",
    "         .when(F.col(\"prob_neg\") - F.col(\"prob_pos\") >= 0.2, -1.0)\n",
    "         .otherwise(0.0)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "irr_sent = ensure_sentiment(irr_base).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dea7658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regexes\n",
    "nonvisit_rx = r\"\"\"(?i)\\b(\n",
    "    never\\s+been|haven'?t\\s+been|didn'?t\\s+visit|did\\s+not\\s+visit|\n",
    "    phone\\s+call|called\\s+(them|store)|left\\s+voicemail|email(ed)?|\n",
    "    website|online\\s+(order|booking|application|support|chat)|\n",
    "    delivery\\s+app|uber\\s*eats|doordash|grab\\s*food|foodpanda\n",
    ")\\b\"\"\"\n",
    "\n",
    "rumor_rx = r\"(?i)\\b(i\\s*(just\\s*)?heard|people\\s+say|someone\\s+told\\s+me|my\\s+friend\\s+said)\\b\"\n",
    "\n",
    "irr_rules = (irr_sent\n",
    "    .withColumn(\"text_nn\", F.coalesce(F.col(\"text\"), F.lit(\"\")))\n",
    "    .withColumn(\"char_len\", F.length(\"text_nn\"))\n",
    "    .withColumn(\"excl_count\", F.size(F.split(F.regexp_replace(\"text_nn\", r\"[^!]\", \"\"), \"\")))\n",
    "    .withColumn(\"nonvisit_clues\", F.col(\"text_nn\").rlike(nonvisit_rx))\n",
    "    .withColumn(\"rumor_clues\",    F.col(\"text_nn\").rlike(rumor_rx))\n",
    "    # core policy flag: negative + (explicit non-visit OR strong proxy)\n",
    "    .withColumn(\"policy_nonvisitor_rant\",\n",
    "        (F.col(\"sentiment_num\") < 0) &\n",
    "        ( F.col(\"nonvisit_clues\") |\n",
    "          F.col(\"rumor_clues\") |\n",
    "          (F.col(\"char_len\") < 40) |           # very short angry blurt\n",
    "          (F.col(\"excl_count\") >= 3)           # lots of exclamation marks\n",
    "        )\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bf5e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/30 22:59:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "def add_key(df):\n",
    "    cols = set(df.columns)\n",
    "    def safe(name, cast=False):\n",
    "        if name in cols:\n",
    "            c = F.col(name)\n",
    "            if cast: c = c.cast(\"string\")\n",
    "            return F.coalesce(c, F.lit(\"\"))\n",
    "        return F.lit(\"\")\n",
    "\n",
    "    return df.withColumn(\"rant_key\", F.sha2(F.concat_ws(\"||\",\n",
    "        safe(\"gmap_id\"),\n",
    "        safe(\"user_id\"),\n",
    "        safe(\"business_name\"),\n",
    "        safe(\"category_str\"),\n",
    "        safe(\"time\", cast=True),\n",
    "        safe(\"text\"),\n",
    "        safe(\"rating\", cast=True)\n",
    "    ), 256))\n",
    "\n",
    "irr_flagged = add_key(irr_rules)\n",
    "\n",
    "rant_only = (irr_flagged\n",
    "    .filter(F.col(\"policy_nonvisitor_rant\"))\n",
    "    .dropDuplicates([\"rant_key\"])     # just in case\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "708eccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rant_keys = rant_only.select(\"rant_key\")\n",
    "\n",
    "irrelevant_no_rant = (irr_flagged.join(rant_keys, on=\"rant_key\", how=\"left_anti\")\n",
    "    .drop(\"text_nn\",\"char_len\",\"excl_count\",\"nonvisit_clues\",\"rumor_clues\",\n",
    "          \"policy_nonvisitor_rant\",\"rant_key\")  # keep your frame clean\n",
    "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a394663f",
   "metadata": {},
   "source": [
    "## Preparing datasets for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d96a4c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant_clean.withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\").printSchema()  # these reviews are to be labelled as \"relevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb8bc1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irrelevant_no_rant.withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\").printSchema()        # these reviews are to be labelled as \"irrelevant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17941d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ads_only.withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\").printSchema()      # these reviews are to be labelled as \"advertisment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef6c6900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rant_only.withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22abb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
