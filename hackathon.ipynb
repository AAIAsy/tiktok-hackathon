{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "590f22ef",
      "metadata": {
        "id": "590f22ef"
      },
      "source": [
        "# **TikTok TechJam 2025 Hackathon (TRACK 1)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f83fc8",
      "metadata": {},
      "source": [
        "**Team Name**: A/B Testing \n",
        "\n",
        "**Member Name(s)**: Asyraf Dzulfiqar, Beata Yeo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea97f6f",
      "metadata": {},
      "source": [
        "# Part 1: Collect data, join them and remove null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5e9c6cd1",
      "metadata": {
        "id": "5e9c6cd1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import numpy as np\n",
        "import os\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, concat_ws, rand, lit\n",
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e29fa5ba",
      "metadata": {
        "id": "e29fa5ba"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "        .appName(\"Hackathon\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"16G\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "        .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.0\") \\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "86fb8052",
      "metadata": {
        "id": "86fb8052"
      },
      "outputs": [],
      "source": [
        "pathing_review = \"datasets/review_data/\"\n",
        "arr = np.array(os.listdir(pathing_review))\n",
        "reviewData_files = pathing_review + arr\n",
        "\n",
        "pathing_metadata = \"datasets/review_metadata/\"\n",
        "arr = np.array(os.listdir(pathing_metadata))\n",
        "reviewMetadata_files = pathing_metadata + arr\n",
        "\n",
        "df_review = spark.read.json(list(reviewData_files)).dropna(subset=\"text\").drop_duplicates()\n",
        "df_metadata = spark.read.json(list(reviewMetadata_files)).dropna(subset=\"category\").drop_duplicates().withColumnRenamed(\"name\", \"business_name\").select([\"gmap_id\", \"category\", \"business_name\"])\n",
        "\n",
        "df_joined = df_review.join(df_metadata, on=\"gmap_id\", how=\"inner\").withColumn(\"category_str\", concat_ws(\", \", col(\"category\"))).withColumn(\"random_order\", rand()).orderBy(\"random_order\").drop(\"random_order\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "Bz_n17punKxh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz_n17punKxh",
        "outputId": "107fabbd-7814-4608-8e2d-e50b07aba11b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2528648"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_joined.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab35bb0",
      "metadata": {
        "id": "cab35bb0",
        "outputId": "730b86d2-cb47-4aae-aed1-8dc9085bc67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- gmap_id: string (nullable = true)\n",
            " |-- category: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- business_name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_metadata.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3a07c7",
      "metadata": {
        "id": "cc3a07c7",
        "outputId": "f0d3c8eb-44ee-4094-eb20-3718b11aa958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- gmap_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- pics: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- url: array (nullable = true)\n",
            " |    |    |    |-- element: string (containsNull = true)\n",
            " |-- rating: long (nullable = true)\n",
            " |-- resp: struct (nullable = true)\n",
            " |    |-- text: string (nullable = true)\n",
            " |    |-- time: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- time: long (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_review.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4d2966",
      "metadata": {
        "id": "ad4d2966",
        "outputId": "4be9b738-6b4a-460d-8ac8-25eccfcb0e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- gmap_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- pics: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- url: array (nullable = true)\n",
            " |    |    |    |-- element: string (containsNull = true)\n",
            " |-- rating: long (nullable = true)\n",
            " |-- resp: struct (nullable = true)\n",
            " |    |-- text: string (nullable = true)\n",
            " |    |-- time: long (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- time: long (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- category: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- business_name: string (nullable = true)\n",
            " |-- category_str: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_joined.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6ec83cf6",
      "metadata": {
        "id": "6ec83cf6"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when, count, size, trim\n",
        "from pyspark.sql.types import StringType, FloatType, DoubleType, ArrayType, MapType\n",
        "\n",
        "# 1) Drop unwanted columns\n",
        "df_filtered = df_joined.drop(\"pics\", \"resp\", \"time\", \"category\", \"user_id\")\n",
        "\n",
        "# 2) Build a \"is missing\" condition per column based on its data type\n",
        "missing_conds = []\n",
        "for f in df_filtered.schema.fields:\n",
        "    c = col(f.name)\n",
        "    dt = f.dataType\n",
        "\n",
        "    if isinstance(dt, (FloatType, DoubleType)):\n",
        "        # floats/doubles: NULL or NaN\n",
        "        cond = c.isNull() | c.isnan()\n",
        "    elif isinstance(dt, StringType):\n",
        "        # strings: NULL or empty after trim\n",
        "        cond = c.isNull() | (trim(c) == \"\")\n",
        "    elif isinstance(dt, (ArrayType, MapType)):\n",
        "        # arrays/maps: NULL or empty\n",
        "        cond = c.isNull() | (size(c) == 0)\n",
        "    else:\n",
        "        # ints/longs/booleans/date/timestamp/structs: only NULL\n",
        "        cond = c.isNull()\n",
        "\n",
        "    missing_conds.append(count(when(cond, True)).alias(f.name))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc7dd97",
      "metadata": {
        "id": "9cc7dd97",
        "outputId": "a14a0a47-fdc5-473e-af53-9187d7bdafde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 12:====================================>                    (9 + 5) / 14]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----+------+----+-------------+------------+\n",
            "|gmap_id|name|rating|text|business_name|category_str|\n",
            "+-------+----+------+----+-------------+------------+\n",
            "|0      |0   |0     |0   |0            |0           |\n",
            "+-------+----+------+----+-------------+------------+\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "missing_counts = df_filtered.select(missing_conds)\n",
        "missing_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c1f72af",
      "metadata": {},
      "source": [
        "# Remove unnecessary text elements"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b014d8d",
      "metadata": {
        "id": "2b014d8d"
      },
      "source": [
        "### Some reviews have translation, we only need English ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e1e7db5",
      "metadata": {
        "id": "5e1e7db5",
        "outputId": "b147c81b-a39d-47a3-c956-8427334917d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 7:==========================================>              (17 + 6) / 23]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                       text|\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "|                                               (Translated by Google) Nice place\\n\\n(Original)\\nLindo lugar|\n",
            "|       (Translated by Google) Visit with respect and respect ..., very nice experience!\\n\\n(Original)\\nM...|\n",
            "|       (Translated by Google) It is a nice place the only detail is that it does not have bathrooms and ...|\n",
            "|(Translated by Google) It's delicious and very good.\\nPromise to go back\\n\\n(Original)\\në§›ìžˆê³  ì•„ì£¼ ì¢‹ì•„...|\n",
            "|       (Translated by Google) Very friendly staff and all their fresh products\\n\\n(Original)\\nEl persona...|\n",
            "+-----------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_joined.filter(df_joined.text.contains(\"(Translated by Google)\")).select(\"text\").show(5, truncate=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1209433",
      "metadata": {
        "id": "c1209433"
      },
      "source": [
        "### Some reviews have many spacings, remove spacings for one-line reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e636a433",
      "metadata": {
        "id": "e636a433",
        "outputId": "9cf0e600-1055-4137-b9b7-1baaf5a6e47a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Stage 13:=========================================>              (17 + 6) / 23]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|          Been doing there a long time. Always get a great haircut -\\nMichelle's the Best!! ðŸ‘ ðŸ‘ ðŸ‘|\n",
            "|Very impressed! Dawn and Jazz are AMAZING!\\nMake your appointments now for all of your holiday ga...|\n",
            "|Awesome customer service. We visited this store for backsplash tiles and one of their design cons...|\n",
            "|[WARNING] outstanding food, highly addictive. The only problem is it attracts a massive amount of...|\n",
            "|BEST BBQ IN PHOENIX!!!\\n\\nStopped Today and it didnâ€™t disappoint!!! It was soo good!!! I got the ...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "df_joined.filter(F.col(\"text\").rlike(\"\\n\")).select(\"text\").show(5, truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ee00a06a",
      "metadata": {
        "id": "ee00a06a"
      },
      "outputs": [],
      "source": [
        "# Removing (Translated by Google) prefix and (Original) languages to get the English reviews\n",
        "# Removing all newlines for one-lined reviews\n",
        "# Removing quotation marks\n",
        "\n",
        "df_joined = df_joined.withColumn(\n",
        "    \"text\",\n",
        "    F.when(\n",
        "        F.col(\"text\").contains(\"(Translated by Google)\"),\n",
        "        # extract the English text, remove newlines, remove quotes\n",
        "        F.regexp_replace(\n",
        "            F.regexp_replace(\n",
        "                F.regexp_extract(F.col(\"text\"), r\"\\(Translated by Google\\)\\s*([^\\n]+)\", 1),\n",
        "                r\"\\n+\", \" \"\n",
        "            ),\n",
        "            r\"\\\"\", \"\"\n",
        "        )\n",
        "    ).otherwise(\n",
        "        # for rows without Google Translate tag, remove newlines and quotes\n",
        "        F.regexp_replace(\n",
        "            F.regexp_replace(F.col(\"text\"), r\"\\n+\", \" \"),\n",
        "            r\"\\\"\", \"\"\n",
        "        )\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536c1b78",
      "metadata": {
        "id": "536c1b78",
        "outputId": "56fc24f3-ecaf-4c48-c3b4-35e9705c3d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning::Spark Session already created, some configs may not take.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "25/08/30 22:58:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
          ]
        }
      ],
      "source": [
        "# spark_nlp = sparknlp.start(apple_silicon=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "41b56c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41b56c96",
        "outputId": "a65a6e02-6fd8-484d-b02c-30462f90b623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# 1. Document Assembler\n",
        "# -------------------------------\n",
        "customer_review = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"customer_review\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Tokenizer\n",
        "# -------------------------------\n",
        "customer_review_token = Tokenizer() \\\n",
        "    .setInputCols([\"customer_review\"]) \\\n",
        "    .setOutputCol(\"customer_review_token\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Spell Checker\n",
        "# -------------------------------\n",
        "customer_review_spell_checker = NorvigSweetingModel.pretrained() \\\n",
        "    .setInputCols([\"customer_review_token\"]) \\\n",
        "    .setOutputCol(\"customer_review_corrected\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Normalizer (lowercasing, clean text)\n",
        "# -------------------------------\n",
        "customer_review_normalizer = Normalizer() \\\n",
        "    .setInputCols([\"customer_review_corrected\"]) \\\n",
        "    .setOutputCol(\"customer_review_normalized\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. StopWords Cleaner\n",
        "# -------------------------------\n",
        "customer_review_stopwordsCleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"customer_review_normalized\"]) \\\n",
        "    .setOutputCol(\"customer_review_cleaned\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Lemmatizer\n",
        "# -------------------------------\n",
        "customer_review_lemma = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"customer_review_token\"]) \\\n",
        "    .setOutputCol(\"customer_review_lemma\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Word Embeddings (GloVe)\n",
        "# -------------------------------\n",
        "glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
        "    .setInputCols([\"customer_review_token\", \"customer_review\"]) \\\n",
        "    .setOutputCol(\"word_embeddings\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Sentence Embeddings (average pooling)\n",
        "# -------------------------------\n",
        "sentence_embeddings = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"customer_review\", \"word_embeddings\"]) \\\n",
        "    .setOutputCol(\"customer_review_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Embeddings Finisher (convert to Spark vector/array)\n",
        "# -------------------------------\n",
        "customer_review_finisher = EmbeddingsFinisher() \\\n",
        "    .setInputCols([\"customer_review_embeddings\"]) \\\n",
        "    .setOutputCols([\"customer_review_vector\"]) \\\n",
        "    .setOutputAsVector(True) \\\n",
        "    .setCleanAnnotations(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5f7d2008",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f7d2008",
        "outputId": "b3fcb278-12ac-4133-a2a1-a3341015fa86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spellcheck_norvig download started this may take some time.\n",
            "Approximate size to download 4.2 MB\n",
            "[OK!]\n",
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------\n",
        "# 1. Document Assembler\n",
        "# -------------------------------\n",
        "business_category = DocumentAssembler() \\\n",
        "    .setInputCol(\"category_str\") \\\n",
        "    .setOutputCol(\"business_category\")\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Tokenizer\n",
        "# -------------------------------\n",
        "business_category_token = Tokenizer() \\\n",
        "    .setInputCols([\"business_category\"]) \\\n",
        "    .setOutputCol(\"business_category_token\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Spell Checker\n",
        "# -------------------------------\n",
        "business_category_spell_checker = NorvigSweetingModel.pretrained() \\\n",
        "    .setInputCols([\"business_category_token\"]) \\\n",
        "    .setOutputCol(\"business_category_corrected\")\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Normalizer (lowercasing, clean text)\n",
        "# -------------------------------\n",
        "business_category_normalizer = Normalizer() \\\n",
        "    .setInputCols([\"business_category_corrected\"]) \\\n",
        "    .setOutputCol(\"business_category_normalized\") \\\n",
        "    .setLowercase(True)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. StopWords Cleaner\n",
        "# -------------------------------\n",
        "business_category_stopwordsCleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"business_category_normalized\"]) \\\n",
        "    .setOutputCol(\"business_category_cleaned\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Lemmatizer\n",
        "# -------------------------------\n",
        "business_category_lemma = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"business_category_token\"]) \\\n",
        "    .setOutputCol(\"business_category_lemma\")\n",
        "\n",
        "# -------------------------------\n",
        "# 7. Word Embeddings (GloVe)\n",
        "# -------------------------------\n",
        "business_category_glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_100d\") \\\n",
        "    .setInputCols([\"business_category_token\", \"business_category\"]) \\\n",
        "    .setOutputCol(\"word_embeddings\")\n",
        "\n",
        "# -------------------------------\n",
        "# 8. Sentence Embeddings (average pooling)\n",
        "# -------------------------------\n",
        "business_category_sentence_embeddings = SentenceEmbeddings() \\\n",
        "    .setInputCols([\"business_category\", \"word_embeddings\"]) \\\n",
        "    .setOutputCol(\"business_category_embeddings\") \\\n",
        "    .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "# -------------------------------\n",
        "# 9. Embeddings Finisher (convert to Spark vector/array)\n",
        "# -------------------------------\n",
        "business_category_finisher = EmbeddingsFinisher() \\\n",
        "    .setInputCols([\"business_category_embeddings\"]) \\\n",
        "    .setOutputCols([\"business_category_vector\"]) \\\n",
        "    .setOutputAsVector(True) \\\n",
        "    .setCleanAnnotations(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "109b9337",
      "metadata": {
        "id": "109b9337"
      },
      "outputs": [],
      "source": [
        "# Combine both into one pipeline\n",
        "pipeline = Pipeline(stages=[\n",
        "    # --- Review branch ---\n",
        "    customer_review,\n",
        "    customer_review_token,\n",
        "    customer_review_spell_checker,\n",
        "    customer_review_normalizer,\n",
        "    customer_review_stopwordsCleaner,\n",
        "    customer_review_lemma,\n",
        "    glove_embeddings,\n",
        "    sentence_embeddings,\n",
        "    customer_review_finisher,\n",
        "\n",
        "    # --- Business category branch ---\n",
        "    business_category,\n",
        "    business_category_token,\n",
        "    business_category_spell_checker,\n",
        "    business_category_normalizer,\n",
        "    business_category_stopwordsCleaner,\n",
        "    business_category_lemma,\n",
        "    business_category_glove_embeddings,\n",
        "    business_category_sentence_embeddings,\n",
        "    business_category_finisher\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2c74523f",
      "metadata": {
        "id": "2c74523f"
      },
      "outputs": [],
      "source": [
        "result = pipeline.fit(df_joined).transform(df_joined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e1de42",
      "metadata": {
        "id": "20e1de42"
      },
      "outputs": [],
      "source": [
        "result.select(\"text\", \"customer_review_vector\").show(5, truncate=50, vertical=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b341bd",
      "metadata": {},
      "source": [
        "# Manual labelling of reviews using statistical means"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d5802c6",
      "metadata": {
        "id": "1d5802c6"
      },
      "source": [
        "### Cosine similarity between review and business category for relevancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "33b56449",
      "metadata": {
        "id": "33b56449"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import udf\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    if v1 is None or v2 is None:\n",
        "        return None\n",
        "    a = np.asarray(v1, dtype=float)\n",
        "    b = np.asarray(v2, dtype=float)\n",
        "\n",
        "    # squeeze 1xN / Nx1 or nested singletons to 1-D\n",
        "    if a.ndim > 1:\n",
        "        a = a.reshape(-1)\n",
        "    if b.ndim > 1:\n",
        "        b = b.reshape(-1)\n",
        "\n",
        "    na = np.linalg.norm(a)\n",
        "    nb = np.linalg.norm(b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return float(np.dot(a, b) / (na * nb))\n",
        "\n",
        "cosine_sim_udf = udf(cosine_similarity, DoubleType())\n",
        "\n",
        "result = result.withColumn(\n",
        "    \"cosine_similarity\",\n",
        "    cosine_sim_udf(\"customer_review_vector\", \"business_category_vector\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f976146e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f976146e",
        "outputId": "187fae16-8b9c-49fe-e375-727a9dba6a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+--------------------------------------------------+------------------+\n",
            "|                                      category_str|                                              text| cosine_similarity|\n",
            "+--------------------------------------------------+--------------------------------------------------+------------------+\n",
            "|                            Window tinting service|I didn't personally go there but I did call and...|0.5424361742267093|\n",
            "|       Bar & grill, American restaurant, Gastropub|Went for their catered/open bar new years event...|0.7932691830051947|\n",
            "|Department store, Clothing store, Craft store, ...|Unlike the Williston Wal-Mart, Minot's Wal-Mart...|0.8120576958394772|\n",
            "|Breakfast restaurant, American restaurant, Brun...|The food, service and ambience we're great. Par...|0.7619881697147413|\n",
            "|                                  Sushi restaurant|There aren't many sushi places in the valley, b...|0.3624278644701128|\n",
            "+--------------------------------------------------+--------------------------------------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"category_str\", \"text\", \"cosine_similarity\").show(5, truncate=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1119a347",
      "metadata": {
        "id": "1119a347"
      },
      "outputs": [],
      "source": [
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "base = (result\n",
        "    .select(\"business_name\",\"category_str\",\"text\",\"rating\",\"cosine_similarity\")\n",
        "    .filter(F.col(\"cosine_similarity\").isNotNull())\n",
        "    .persist(StorageLevel.MEMORY_AND_DISK))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7WMbMrVup2hx",
      "metadata": {
        "id": "7WMbMrVup2hx"
      },
      "outputs": [],
      "source": [
        "def cache_once(df, level=StorageLevel.MEMORY_AND_DISK):\n",
        "    # only cache if this instance isnâ€™t already cached\n",
        "    if not df.is_cached:\n",
        "        df = df.persist(level)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8XHxn8K1p0TV",
      "metadata": {
        "id": "8XHxn8K1p0TV"
      },
      "outputs": [],
      "source": [
        "test = result.select(\"cosine_similarity\").filter(F.col(\"cosine_similarity\").isNotNull())\n",
        "\n",
        "# Global quantiles\n",
        "q = test.approxQuantile(\"cosine_similarity\", [0.25, 0.5, 0.75], 0.01)\n",
        "LO, MED, HI = q[0], q[1], q[2]\n",
        "print(\"LO, MED, HI =\", LO, MED, HI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "810225ce",
      "metadata": {
        "id": "810225ce"
      },
      "outputs": [],
      "source": [
        "benchmark = 0.70\n",
        "low = 0.60\n",
        "\n",
        "relevant_df  = cache_once(base.filter(F.col(\"cosine_similarity\") >= benchmark))\n",
        "irrelevant_df = cache_once(base.filter(F.col(\"cosine_similarity\") <= low))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e854f47",
      "metadata": {
        "id": "8e854f47"
      },
      "source": [
        "### Operations to find promotional links or advertisments in both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "de023fc1",
      "metadata": {
        "id": "de023fc1"
      },
      "outputs": [],
      "source": [
        "# Helper funtions\n",
        "\n",
        "def add_ad_key(df):\n",
        "    cols = set(df.columns)\n",
        "\n",
        "    def safe(name, cast_str=False):\n",
        "        if name in cols:\n",
        "            c = F.col(name)\n",
        "            if cast_str:\n",
        "                c = c.cast(\"string\")\n",
        "            return F.coalesce(c, F.lit(\"\"))\n",
        "        else:\n",
        "            return F.lit(\"\")\n",
        "\n",
        "    # Build a stable key from available fields (order matters).\n",
        "    # Include text + business_name + category_str + rating + cosine_similarity; add ids/time if present.\n",
        "    return df.withColumn(\n",
        "        \"ad_key\",\n",
        "        F.sha2(F.concat_ws(\"||\",\n",
        "            safe(\"gmap_id\"),\n",
        "            safe(\"user_id\"),\n",
        "            safe(\"business_name\"),\n",
        "            safe(\"category_str\"),\n",
        "            safe(\"time\", cast_str=True),\n",
        "            safe(\"text\"),\n",
        "            safe(\"rating\", cast_str=True),\n",
        "            safe(\"cosine_similarity\", cast_str=True)\n",
        "        ), 256)\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "def with_ads_flags(df):\n",
        "    txt = F.coalesce(F.col(\"text\"), F.lit(\"\"))\n",
        "\n",
        "    return (df\n",
        "      # URLs / domains / shorteners / obfuscations\n",
        "      .withColumn(\"has_url\",          txt.rlike(r\"(?i)\\bhttps?://\\S+|\\bwww\\.\\S+\"))\n",
        "      .withColumn(\"has_domain\",       txt.rlike(r\"(?i)\\b[a-z0-9][a-z0-9\\-]*\\.(?:com|net|org|co|io|info|biz|app|shop|store|sg|uk|au|ca|de|fr|my|ph|id|in)(?:/\\S*)?\\b\"))\n",
        "      .withColumn(\"has_shortener\",    txt.rlike(r\"(?i)\\b(bit\\.ly|t\\.co|goo\\.gl|tinyurl\\.com|ow\\.ly|wa\\.me|linktr\\.ee)/\\S+\"))\n",
        "      .withColumn(\"has_obfus_domain\", txt.rlike(r\"(?i)\\b[a-z0-9][a-z0-9\\-]*\\s*(?:\\.|dot|\\[\\.]|\\(dot\\))\\s*(?:com|net|org|co|io|sg|au|uk)\\b\"))\n",
        "\n",
        "      # Contact info / WhatsApp\n",
        "      .withColumn(\"has_email\",        txt.rlike(r\"(?i)[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\"))\n",
        "      .withColumn(\"has_phone\",        txt.rlike(r\"(?i)(?:\\+?\\d[\\s\\-().]{0,3}){7,}\\d\"))\n",
        "      .withColumn(\"has_whatsapp\",     txt.rlike(r\"(?i)\\bwhatsapp\\b|\\bwa\\.me/\\S+\"))\n",
        "\n",
        "      # Promo / CTA phrases\n",
        "      .withColumn(\"has_promo_words\",  txt.rlike(r\"(?i)\\b(promo(?:\\s*code)?|discount(?:\\s*code)?|coupon|use\\s+code|deal|sale|flash\\s*sale|limited\\s*time|special\\s*offer|[0-9]{1,3}%\\s*off|buy\\s*now|order\\s*now|book\\s*now|free\\s*shipping|visit\\s+(?:our\\s+)?website|click\\s+(?:here|link))\\b\"))\n",
        "\n",
        "      # Final flag + triggers for auditability\n",
        "      .withColumn(\"policy_ads\",\n",
        "          F.col(\"has_url\") | F.col(\"has_domain\") | F.col(\"has_shortener\") |\n",
        "          F.col(\"has_obfus_domain\") | F.col(\"has_email\") | F.col(\"has_phone\") |\n",
        "          F.col(\"has_whatsapp\") | F.col(\"has_promo_words\")\n",
        "      )\n",
        "      .withColumn(\"ads_triggers\", F.array_remove(F.array(\n",
        "          F.when(F.col(\"has_url\"),          F.lit(\"url\")),\n",
        "          F.when(F.col(\"has_domain\"),       F.lit(\"domain\")),\n",
        "          F.when(F.col(\"has_shortener\"),    F.lit(\"shortener\")),\n",
        "          F.when(F.col(\"has_obfus_domain\"), F.lit(\"obfus_domain\")),\n",
        "          F.when(F.col(\"has_email\"),        F.lit(\"email\")),\n",
        "          F.when(F.col(\"has_phone\"),        F.lit(\"phone\")),\n",
        "          F.when(F.col(\"has_whatsapp\"),     F.lit(\"whatsapp\")),\n",
        "          F.when(F.col(\"has_promo_words\"),  F.lit(\"promo_words\"))\n",
        "      ), None))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7c814c9",
      "metadata": {
        "id": "b7c814c9"
      },
      "outputs": [],
      "source": [
        "relevant_flagged = add_ad_key(with_ads_flags(relevant_df))\n",
        "irrelevant_flagged = add_ad_key(with_ads_flags(irrelevant_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a782a702",
      "metadata": {
        "id": "a782a702"
      },
      "outputs": [],
      "source": [
        "ads_union = (relevant_flagged.withColumn(\"source_split\", F.lit(\"relevant\"))\n",
        "             .unionByName(irrelevant_flagged.withColumn(\"source_split\", F.lit(\"irrelevant\"))))\n",
        "\n",
        "ads_only = (ads_union\n",
        "            .filter(F.col(\"policy_ads\"))\n",
        "            .dropDuplicates([\"ad_key\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b195e227",
      "metadata": {
        "id": "b195e227"
      },
      "outputs": [],
      "source": [
        "ads_keys = ads_only.select(\"ad_key\")\n",
        "\n",
        "relevant_clean = (relevant_flagged.join(ads_keys, on=\"ad_key\", how=\"left_anti\")\n",
        "                  .drop(\"has_url\",\"has_domain\",\"has_shortener\",\"has_obfus_domain\",\n",
        "                        \"has_email\",\"has_phone\",\"has_whatsapp\",\"has_promo_words\",\n",
        "                        \"policy_ads\",\"ads_triggers\"))\n",
        "\n",
        "irrelevant_clean = (irrelevant_flagged.join(ads_keys, on=\"ad_key\", how=\"left_anti\")\n",
        "                    .drop(\"has_url\",\"has_domain\",\"has_shortener\",\"has_obfus_domain\",\n",
        "                          \"has_email\",\"has_phone\",\"has_whatsapp\",\"has_promo_words\",\n",
        "                          \"policy_ads\",\"ads_triggers\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5859d769",
      "metadata": {
        "id": "5859d769"
      },
      "source": [
        "### Find reviews that are rants without visit in irrelevant dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "51fc63d7",
      "metadata": {
        "id": "51fc63d7"
      },
      "outputs": [],
      "source": [
        "# Choose the base irrelevant dataframe safely\n",
        "try:\n",
        "    irr_base = irrelevant_clean\n",
        "except NameError:\n",
        "    try:\n",
        "        irr_base = irrelevant_df\n",
        "    except NameError:\n",
        "        irr_base = irrelevant_df\n",
        "\n",
        "irr_base = irr_base.cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "18ccfee6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18ccfee6",
        "outputId": "aac7ea7e-214b-44f5-c1b6-ce5895158283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentiment_vivekn download started this may take some time.\n",
            "Approximate size to download 873.6 KB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "def ensure_sentiment(df):\n",
        "    if \"sentiment_num\" in df.columns:\n",
        "        return df\n",
        "\n",
        "    # Prefer corrected tokens if available\n",
        "    use_corr = \"customer_review_corrected\" in df.columns\n",
        "    text_col = \"text\"\n",
        "\n",
        "    if use_corr:\n",
        "        from sparknlp.base import Finisher\n",
        "        df = (Finisher()\n",
        "              .setInputCols([\"customer_review_corrected\"])\n",
        "              .setOutputCols([\"corr_tokens\"])\n",
        "              .setOutputAsArray(True)\n",
        "              .setCleanAnnotations(True)\n",
        "             ).transform(df)\n",
        "        df = df.withColumn(\"text_corrected\", F.array_join(\"corr_tokens\", \" \"))\n",
        "        text_col = \"text_corrected\"\n",
        "\n",
        "\n",
        "\n",
        "    document = DocumentAssembler().setInputCol(text_col).setOutputCol(\"doc\")\n",
        "    token    = Tokenizer().setInputCols([\"doc\"]).setOutputCol(\"tok\")\n",
        "    viv      = ViveknSentimentModel.pretrained().setInputCols([\"doc\",\"tok\"]).setOutputCol(\"sent\")\n",
        "    pipe     = Pipeline(stages=[document, token, viv]).fit(df)\n",
        "\n",
        "    out = pipe.transform(df)\n",
        "    out = (out\n",
        "        .withColumn(\"sentiment_str\", F.expr(\"sent[0].result\"))\n",
        "        .withColumn(\"prob_pos\", F.expr(\"cast(sent[0].metadata['positive'] as double)\"))\n",
        "        .withColumn(\"prob_neg\", F.expr(\"cast(sent[0].metadata['negative'] as double)\"))\n",
        "        .drop(\"sent\")\n",
        "    )\n",
        "    # margin to avoid shaky labels\n",
        "    out = out.withColumn(\n",
        "        \"sentiment_num\",\n",
        "        F.when(F.col(\"prob_pos\") - F.col(\"prob_neg\") >= 0.2, 1.0)\n",
        "         .when(F.col(\"prob_neg\") - F.col(\"prob_pos\") >= 0.2, -1.0)\n",
        "         .otherwise(0.0)\n",
        "    )\n",
        "    return out\n",
        "\n",
        "irr_sent = ensure_sentiment(irr_base).cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9dea7658",
      "metadata": {
        "id": "9dea7658"
      },
      "outputs": [],
      "source": [
        "# Regexes\n",
        "nonvisit_rx = r\"\"\"(?i)\\b(\n",
        "    never\\s+been|haven'?t\\s+been|didn'?t\\s+visit|did\\s+not\\s+visit|\n",
        "    phone\\s+call|called\\s+(them|store)|left\\s+voicemail|email(ed)?|\n",
        "    website|online\\s+(order|booking|application|support|chat)|\n",
        "    delivery\\s+app|uber\\s*eats|doordash|grab\\s*food|foodpanda\n",
        ")\\b\"\"\"\n",
        "\n",
        "rumor_rx = r\"(?i)\\b(i\\s*(just\\s*)?heard|people\\s+say|someone\\s+told\\s+me|my\\s+friend\\s+said)\\b\"\n",
        "\n",
        "irr_rules = (irr_sent\n",
        "    .withColumn(\"text_nn\", F.coalesce(F.col(\"text\"), F.lit(\"\")))\n",
        "    .withColumn(\"char_len\", F.length(\"text_nn\"))\n",
        "    .withColumn(\"excl_count\", F.size(F.split(F.regexp_replace(\"text_nn\", r\"[^!]\", \"\"), \"\")))\n",
        "    .withColumn(\"nonvisit_clues\", F.col(\"text_nn\").rlike(nonvisit_rx))\n",
        "    .withColumn(\"rumor_clues\",    F.col(\"text_nn\").rlike(rumor_rx))\n",
        "    # core policy flag: negative + (explicit non-visit OR strong proxy)\n",
        "    .withColumn(\"policy_nonvisitor_rant\",\n",
        "        (F.col(\"sentiment_num\") < 0) &\n",
        "        ( F.col(\"nonvisit_clues\") |\n",
        "          F.col(\"rumor_clues\") |\n",
        "          (F.col(\"char_len\") < 40) |           # very short angry blurt\n",
        "          (F.col(\"excl_count\") >= 3)           # lots of exclamation marks\n",
        "        )\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1bf5e9f9",
      "metadata": {
        "id": "1bf5e9f9"
      },
      "outputs": [],
      "source": [
        "def add_key(df):\n",
        "    cols = set(df.columns)\n",
        "    def safe(name, cast=False):\n",
        "        if name in cols:\n",
        "            c = F.col(name)\n",
        "            if cast: c = c.cast(\"string\")\n",
        "            return F.coalesce(c, F.lit(\"\"))\n",
        "        return F.lit(\"\")\n",
        "\n",
        "    return df.withColumn(\"rant_key\", F.sha2(F.concat_ws(\"||\",\n",
        "        safe(\"gmap_id\"),\n",
        "        safe(\"user_id\"),\n",
        "        safe(\"business_name\"),\n",
        "        safe(\"category_str\"),\n",
        "        safe(\"time\", cast=True),\n",
        "        safe(\"text\"),\n",
        "        safe(\"rating\", cast=True)\n",
        "    ), 256))\n",
        "\n",
        "irr_flagged = add_key(irr_rules)\n",
        "\n",
        "rant_only = (irr_flagged\n",
        "    .filter(F.col(\"policy_nonvisitor_rant\"))\n",
        "    .dropDuplicates([\"rant_key\"])     # just in case\n",
        "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "708eccaf",
      "metadata": {
        "id": "708eccaf"
      },
      "outputs": [],
      "source": [
        "rant_keys = rant_only.select(\"rant_key\")\n",
        "\n",
        "irrelevant_no_rant = (irr_flagged.join(rant_keys, on=\"rant_key\", how=\"left_anti\")\n",
        "    .drop(\"text_nn\",\"char_len\",\"excl_count\",\"nonvisit_clues\",\"rumor_clues\",\n",
        "          \"policy_nonvisitor_rant\",\"rant_key\")  # keep your frame clean\n",
        "    .persist(StorageLevel.MEMORY_AND_DISK)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9510269",
      "metadata": {},
      "source": [
        "# Model training pipeline setup and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a394663f",
      "metadata": {
        "id": "a394663f"
      },
      "source": [
        "## Preparing datasets for BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d96a4c4b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d96a4c4b",
        "outputId": "c34b47d0-0c50-4407-b2b0-dc21e8d5f7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "relevant_dataset = relevant_clean.withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\").limit(1500)\n",
        "relevant_dataset.printSchema()  # these reviews are to be labelled as \"relevant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cb8bc1f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb8bc1f6",
        "outputId": "c3176b21-76cd-48ff-d918-455826eb59bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "irrelevant_dataset = irrelevant_no_rant.withColumn(\"label\", lit(\"IRRELEVANT\")).select(\"text\", \"label\").limit(1500)\n",
        "irrelevant_dataset.printSchema()        # these reviews are to be labelled as \"irrelevant\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "17941d1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17941d1e",
        "outputId": "0ea54761-55d6-4322-fcb6-84d4478d3a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ads_dataset = ads_only.withColumn(\"label\", lit(\"ADVERTISMENT\")).select(\"text\", \"label\").limit(1500)\n",
        "ads_dataset.printSchema()      # these reviews are to be labelled as \"advertisment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ef6c6900",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef6c6900",
        "outputId": "1fb76e98-f49f-4d9d-fe30-d3fcea3a282f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rant_dataset = rant_only.withColumn(\"label\", lit(\"RANT WITHOUT VISIT\")).select(\"text\", \"label\").limit(1500)\n",
        "rant_dataset.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "iLMo7hQzZHbX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLMo7hQzZHbX",
        "outputId": "03b33cab-0390-4258-8308-d5ebb17b315a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1500, 1500, 1500, 0)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "relevant_dataset.count(), irrelevant_dataset.count(), ads_dataset.count(), rant_dataset.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c664f5dd",
      "metadata": {
        "id": "c664f5dd"
      },
      "source": [
        "## Setting up model pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "78039065",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78039065",
        "outputId": "fabf4a83-1f6d-4456-94a3-984a9a9ad804"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[text: string, label: string]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all = relevant_dataset.unionByName(irrelevant_dataset).unionByName(ads_dataset).unionByName(rant_dataset)\n",
        "\n",
        "train_df, test_df = df_all.randomSplit([0.8, 0.2], seed=42)\n",
        "train_df.cache()\n",
        "test_df.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96b19fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d96b19fb",
        "outputId": "19c4395c-791c-4f48-cd5d-736a5c0693b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sent_small_bert_L2_128 download started this may take some time.\n",
            "Approximate size to download 16.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import BertSentenceEmbeddings, ClassifierDLApproach\n",
        "\n",
        "# 4a) Turn raw text into 'document'\n",
        "document_assembler = (DocumentAssembler()\n",
        "    .setInputCol(\"text\")\n",
        "    .setOutputCol(\"document\")\n",
        ")\n",
        "\n",
        "# 4b) Sentence-level BERT embeddings\n",
        "sentence_bert = (BertSentenceEmbeddings\n",
        "    .pretrained(\"sent_small_bert_L2_128\", \"en\")   \n",
        "    .setInputCols([\"document\"])\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        ")\n",
        "\n",
        "# 4c) Classification head\n",
        "classifier = (ClassifierDLApproach()\n",
        "    .setInputCols([\"sentence_embeddings\"])\n",
        "    .setOutputCol(\"class\")          \n",
        "    .setLabelColumn(\"label\")       \n",
        "    .setBatchSize(32)\n",
        "    .setMaxEpochs(10)\n",
        "    .setLr(1e-3)\n",
        "    .setValidationSplit(0.1)\n",
        "    .setEnableOutputLogs(True)      \n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[document_assembler, sentence_bert, classifier])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "098921a2",
      "metadata": {
        "id": "098921a2"
      },
      "outputs": [],
      "source": [
        "model = pipeline.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "9f894e97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f894e97",
        "outputId": "2eb7b1a1-f136-444d-9a5f-2627168c4b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------+--------+------------+\n",
            "|                                                                            text|   label|  pred_label|\n",
            "+--------------------------------------------------------------------------------+--------+------------+\n",
            "|**Update** See below Nice experience. Several pools, nice selection of temper...|RELEVANT|ADVERTISMENT|\n",
            "|4 years leaving here and I have to say that is very nice and secure place... ...|RELEVANT|ADVERTISMENT|\n",
            "|                           8.99 per pound for the best slice roast beef in town!|RELEVANT|ADVERTISMENT|\n",
            "|A bit out dated and very strong odor of air fresheners which was as little ov...|RELEVANT|  IRRELEVANT|\n",
            "|A hidden gem in Billings, feels like a small town diner, the people are super...|RELEVANT|    RELEVANT|\n",
            "+--------------------------------------------------------------------------------+--------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "pred = model.transform(test_df)\n",
        "\n",
        "# Extract the top prediction string from the annotation column 'class'\n",
        "pred = pred.withColumn(\"pred_label\", F.col(\"class.result\").getItem(0))\n",
        "pred.select(\"text\", \"label\", \"pred_label\").show(5, truncate=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "NKjbxavAUm-G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKjbxavAUm-G",
        "outputId": "0db86bd1-62ad-46d9-e926-cb31631c7fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7064\n",
            "F1:        0.7050\n",
            "Precision: 0.7231\n",
            "Recall:    0.7064\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Fit on the union of both columns to guarantee identical mapping\n",
        "label_union = (pred\n",
        "    .select(F.col(\"label\").alias(\"lab\"))\n",
        "    .union(pred.select(F.col(\"pred_label\").alias(\"lab\")))\n",
        "    .distinct()\n",
        ")\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"lab\", outputCol=\"lab_idx\", handleInvalid=\"keep\")\n",
        "idx_model_base = indexer.fit(label_union)\n",
        "\n",
        "# Reuse the same fitted model for each column via copy with new params\n",
        "idx_true = idx_model_base.copy({idx_model_base.inputCol: \"label\",\n",
        "                                idx_model_base.outputCol: \"label_idx\"})\n",
        "idx_pred = idx_model_base.copy({idx_model_base.inputCol: \"pred_label\",\n",
        "                                idx_model_base.outputCol: \"prediction\"})\n",
        "\n",
        "# Apply both models\n",
        "eval_df = idx_true.transform(pred)\n",
        "eval_df = idx_pred.transform(eval_df).select(\"label_idx\", \"prediction\")\n",
        "\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_idx\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
        ").evaluate(eval_df)\n",
        "\n",
        "f1 = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_idx\", predictionCol=\"prediction\", metricName=\"f1\"\n",
        ").evaluate(eval_df)\n",
        "\n",
        "pr = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_idx\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
        ").evaluate(eval_df)\n",
        "\n",
        "rc = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label_idx\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
        ").evaluate(eval_df)\n",
        "\n",
        "print(f\"Accuracy:  {acc:.4f}\")\n",
        "print(f\"F1:        {f1:.4f}\")\n",
        "print(f\"Precision: {pr:.4f}\")\n",
        "print(f\"Recall:    {rc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "64e81baa",
      "metadata": {
        "id": "64e81baa"
      },
      "outputs": [],
      "source": [
        "# Save\n",
        "model_path = \"bert_cls_model\"\n",
        "model.write().overwrite().save(model_path)\n",
        "\n",
        "# Load later\n",
        "# from pyspark.ml import PipelineModel\n",
        "# loaded_model = PipelineModel.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "mDFvid-IXjSw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDFvid-IXjSw",
        "outputId": "54fd9ac1-ceba-4518-9de9-a2072277558e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+---+---+---+\n",
            "|y  |0.0|1.0|2.0|\n",
            "+---+---+---+---+\n",
            "|0.0|273|4  |32 |\n",
            "|1.0|19 |165|114|\n",
            "|2.0|45 |40 |173|\n",
            "+---+---+---+---+\n",
            "\n",
            "+---+---+---+------+------------------+------------------+------------------+\n",
            "|y  |tp |n_y|n_yhat|precision         |recall            |f1                |\n",
            "+---+---+---+------+------------------+------------------+------------------+\n",
            "|2.0|173|258|319   |0.542319749216301 |0.6705426356589147|0.5996533795493935|\n",
            "|1.0|165|298|209   |0.7894736842105263|0.5536912751677853|0.650887573964497 |\n",
            "|0.0|273|309|337   |0.8100890207715133|0.883495145631068 |0.8452012383900929|\n",
            "+---+---+---+------+------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit ONE indexer on union so mapping matches for both cols\n",
        "lab_union = (pred.select(F.col(\"label\").alias(\"lab\"))\n",
        "                  .union(pred.select(F.col(\"pred_label\").alias(\"lab\")))\n",
        "                  .distinct())\n",
        "base_idx = StringIndexer(inputCol=\"lab\", outputCol=\"lab_idx\", handleInvalid=\"keep\").fit(lab_union)\n",
        "idx_true = base_idx.copy({base_idx.inputCol: \"label\",      base_idx.outputCol: \"y\"})\n",
        "idx_pred = base_idx.copy({base_idx.inputCol: \"pred_label\", base_idx.outputCol: \"yhat\"})\n",
        "\n",
        "e = idx_pred.transform(idx_true.transform(pred)).select(\"y\",\"yhat\",\"label\",\"pred_label\")\n",
        "\n",
        "# Confusion matrix (pivoted)\n",
        "cm = (e.groupBy(\"y\").pivot(\"yhat\").agg(F.count(\"*\")).na.fill(0).orderBy(\"y\"))\n",
        "cm.show(50, truncate=False)\n",
        "\n",
        "# Per-class metrics\n",
        "tot_by_y    = e.groupBy(\"y\").agg(F.count(\"*\").alias(\"n_y\"))\n",
        "tot_by_yhat = e.groupBy(\"yhat\").agg(F.count(\"*\").alias(\"n_yhat\"))\n",
        "tp = e.filter(F.col(\"y\")==F.col(\"yhat\")).groupBy(\"y\").agg(F.count(\"*\").alias(\"tp\"))\n",
        "\n",
        "perclass = (tp.join(tot_by_y, \"y\").join(tot_by_yhat, tp[\"y\"]==tot_by_yhat[\"yhat\"], \"left\")\n",
        "              .drop(\"yhat\")\n",
        "              .withColumn(\"precision\", F.col(\"tp\")/F.col(\"n_yhat\"))\n",
        "              .withColumn(\"recall\",    F.col(\"tp\")/F.col(\"n_y\"))\n",
        "              .withColumn(\"f1\",        2*F.col(\"precision\")*F.col(\"recall\")/(F.col(\"precision\")+F.col(\"recall\"))))\n",
        "perclass.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "KgFtaH9_Y8Xl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFtaH9_Y8Xl",
        "outputId": "a76d7fb0-8f33-4e28-b5af-8a94291ba6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index -> label: [(0, 'ADVERTISMENT'), (1, 'IRRELEVANT'), (2, 'RELEVANT')]\n"
          ]
        }
      ],
      "source": [
        "# which string label corresponds to index 0,1,2 ?\n",
        "print(\"index -> label:\", list(enumerate(base_idx.labels)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9SFMHQUqbfSB",
      "metadata": {
        "id": "9SFMHQUqbfSB"
      },
      "source": [
        "### Analysis on datasets to understand why the models would label reviews as such"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "VeiUtzW0bllE",
      "metadata": {
        "id": "VeiUtzW0bllE"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "def _get(name): return globals().get(name, None)\n",
        "\n",
        "dfs = {\n",
        "    \"RELEVANT\": _get(\"relevant_clean\"),\n",
        "    \"IRRELEVANT\": _get(\"irrelevant_no_rant\"),\n",
        "    \"ADVERTISEMENT\": _get(\"ads_only\"),\n",
        "    \"RANT WITHOUT VISIT\": _get(\"rant_only\"),\n",
        "}\n",
        "missing = [k for k,v in dfs.items() if v is None]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing DataFrames: {missing}. Create those first.\")\n",
        "\n",
        "union_cols = []\n",
        "for label, df in dfs.items():\n",
        "    # keep only text and add label\n",
        "    union_cols.append(df.select(F.col(\"text\").alias(\"text\"), F.lit(label).alias(\"label\")))\n",
        "\n",
        "labeled = union_cols[0]\n",
        "for part in union_cols[1:]:\n",
        "    labeled = labeled.unionByName(part)\n",
        "\n",
        "# basic hygiene\n",
        "labeled = (labeled\n",
        "    .withColumn(\"text\", F.col(\"text\").cast(\"string\"))\n",
        "    .filter(F.col(\"text\").isNotNull() & (F.length(\"text\") > 0))\n",
        ")\n",
        "\n",
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "tok = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\",\n",
        "                     pattern=r\"[^A-Za-z0-9]+\", toLowercase=True, minTokenLength=2)\n",
        "rm  = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_clean\")\n",
        "\n",
        "pipe = Pipeline(stages=[tok, rm]).fit(labeled)\n",
        "cleaned = pipe.transform(labeled).select(\"label\",\"tokens_clean\").filter(F.size(\"tokens_clean\") > 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "7pY_oWJZb5pA",
      "metadata": {
        "id": "7pY_oWJZb5pA"
      },
      "outputs": [],
      "source": [
        "# Document frequency: use per-doc unique tokens\n",
        "docs_unique = cleaned.select(F.array_distinct(\"tokens_clean\").alias(\"utoks\"))\n",
        "N_docs = docs_unique.count()\n",
        "\n",
        "idf = (docs_unique\n",
        "       .select(F.explode(\"utoks\").alias(\"term\"))\n",
        "       .groupBy(\"term\").agg(F.count(\"*\").alias(\"df\"))\n",
        "       .withColumn(\"idf\", F.log( (F.lit(N_docs)+1.0) / (F.col(\"df\")+1.0) ) + F.lit(1.0))\n",
        ")\n",
        "\n",
        "# Per-label term frequency\n",
        "tf_label = (cleaned\n",
        "    .select(\"label\", F.explode(\"tokens_clean\").alias(\"term\"))\n",
        "    .groupBy(\"label\",\"term\").agg(F.count(\"*\").alias(\"tf\"))\n",
        ")\n",
        "\n",
        "# TF-IDF per label\n",
        "tfidf_label = (tf_label.join(idf, on=\"term\", how=\"inner\")\n",
        "               .withColumn(\"score\", F.col(\"tf\") * F.col(\"idf\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "ScnfYgEedam2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScnfYgEedam2",
        "outputId": "3a31b650-a3a0-4186-98e5-00563916e3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+----------+------+------------------+------------------+----+\n",
            "|label        |term      |tf    |idf               |score             |rank|\n",
            "+-------------+----------+------+------------------+------------------+----+\n",
            "|ADVERTISEMENT|deal      |6893  |6.233113881852588 |42964.85398760989 |1   |\n",
            "|ADVERTISEMENT|sale      |4678  |6.649039096785642 |31104.204894763232|2   |\n",
            "|ADVERTISEMENT|discount  |2956  |7.1034613182966675|20997.831656884948|3   |\n",
            "|ADVERTISEMENT|get       |3768  |3.839536536518086 |14467.373669600147|4   |\n",
            "|ADVERTISEMENT|great     |5968  |2.339466177097054 |13961.93414491522 |5   |\n",
            "|ADVERTISEMENT|good      |5090  |2.5770313395532987|13117.089518326291|6   |\n",
            "|ADVERTISEMENT|store     |2968  |4.010623399241094 |11903.530248947567|7   |\n",
            "|ADVERTISEMENT|coupon    |1403  |7.917115589588923 |11107.71317219326 |8   |\n",
            "|ADVERTISEMENT|one       |2711  |4.093896967715667 |11098.554679477173|9   |\n",
            "|ADVERTISEMENT|time      |2671  |3.8951598854889107|10403.97205414088 |10  |\n",
            "|ADVERTISEMENT|got       |2146  |4.595517751400719 |9861.981094505942 |11  |\n",
            "|ADVERTISEMENT|like      |2265  |4.035804915618521 |9141.09813387595  |12  |\n",
            "|ADVERTISEMENT|always    |2495  |3.5321065001477563|8812.605717868651 |13  |\n",
            "|ADVERTISEMENT|go        |2252  |3.9065885813125334|8797.637485115825 |14  |\n",
            "|ADVERTISEMENT|service   |3125  |2.8056455539997467|8767.64235624921  |15  |\n",
            "|ADVERTISEMENT|us        |1695  |5.024836214272741 |8517.097383192297 |16  |\n",
            "|ADVERTISEMENT|back      |1905  |4.296863747514095 |8185.525439014351 |17  |\n",
            "|ADVERTISEMENT|also      |1712  |4.636364361002635 |7937.455786036511 |18  |\n",
            "|ADVERTISEMENT|items     |1510  |5.255575012675844 |7935.918269140524 |19  |\n",
            "|ADVERTISEMENT|even      |1705  |4.645855412725227 |7921.183478696513 |20  |\n",
            "|ADVERTISEMENT|price     |1677  |4.7202812302247725|7915.911623086943 |21  |\n",
            "|ADVERTISEMENT|place     |2596  |2.9767423845341865|7727.623230250748 |22  |\n",
            "|ADVERTISEMENT|prices    |1872  |4.096960251046184 |7669.509589958457 |23  |\n",
            "|ADVERTISEMENT|car       |1270  |5.472565049045521 |6950.157612287811 |24  |\n",
            "|ADVERTISEMENT|food      |2772  |2.4882689796596167|6897.4816116164575|25  |\n",
            "|ADVERTISEMENT|new       |1433  |4.805274712918867 |6885.958663612736 |26  |\n",
            "|ADVERTISEMENT|went      |1407  |4.840610672135478 |6810.739215694617 |27  |\n",
            "|ADVERTISEMENT|said      |1200  |5.616578418670444 |6739.8941024045325|28  |\n",
            "|ADVERTISEMENT|staff     |2056  |3.274291761802232 |6731.943862265389 |29  |\n",
            "|ADVERTISEMENT|told      |1139  |5.803584979718618 |6610.283291899506 |30  |\n",
            "|IRRELEVANT   |great     |114831|2.339466177097054 |268643.24058223184|1   |\n",
            "|IRRELEVANT   |good      |80027 |2.5770313395532987|206232.08701043183|2   |\n",
            "|IRRELEVANT   |place     |60918 |2.9767423845341865|181337.19258105356|3   |\n",
            "|IRRELEVANT   |food      |71962 |2.4882689796596167|179060.81231426535|4   |\n",
            "|IRRELEVANT   |service   |50665 |2.8056455539997467|142148.03199339716|5   |\n",
            "|IRRELEVANT   |nice      |38691 |3.4440469094848103|133253.6189748768 |6   |\n",
            "|IRRELEVANT   |friendly  |38434 |3.2975729159884994|126738.91745310198|7   |\n",
            "|IRRELEVANT   |staff     |37645 |3.274291761802232 |123260.71337304502|8   |\n",
            "|IRRELEVANT   |love      |28220 |3.689278847185948 |104111.44906758745|9   |\n",
            "|IRRELEVANT   |always    |28765 |3.5321065001477563|101601.04347675022|10  |\n",
            "|IRRELEVANT   |best      |24016 |3.7373919602300543|89757.20531688498 |11  |\n",
            "|IRRELEVANT   |awesome   |21024 |4.2414988200969805|89173.27119371892 |12  |\n",
            "|IRRELEVANT   |clean     |20210 |3.9683929866753864|80201.22226070956 |13  |\n",
            "|IRRELEVANT   |time      |20349 |3.8951598854889107|79262.60850981384 |14  |\n",
            "|IRRELEVANT   |people    |18928 |4.156981775074119 |78683.35103860292 |15  |\n",
            "|IRRELEVANT   |go        |20111 |3.9065885813125334|78565.40295877637 |16  |\n",
            "|IRRELEVANT   |get       |19775 |3.839536536518086 |75926.83500964515 |17  |\n",
            "|IRRELEVANT   |amazing   |16228 |4.3896725664607334|71235.60640852478 |18  |\n",
            "|IRRELEVANT   |prices    |16538 |4.096960251046184 |67755.52863180179 |19  |\n",
            "|IRRELEVANT   |like      |16600 |4.035804915618521 |66994.36159926745 |20  |\n",
            "|IRRELEVANT   |excellent |14946 |4.314236269739693 |64480.57528752945 |21  |\n",
            "|IRRELEVANT   |back      |14097 |4.296863747514095 |60572.8882487062  |22  |\n",
            "|IRRELEVANT   |really    |13828 |4.323149503675753 |59780.511336828306|23  |\n",
            "|IRRELEVANT   |one       |14476 |4.093896967715667 |59263.25250465199 |24  |\n",
            "|IRRELEVANT   |helpful   |13552 |4.318056095009093 |58518.296199563236|25  |\n",
            "|IRRELEVANT   |atmosphere|12591 |4.556047134799741 |57365.189474263534|26  |\n",
            "|IRRELEVANT   |selection |12918 |4.403057249475813 |56878.69354872855 |27  |\n",
            "|IRRELEVANT   |fun       |11463 |4.755702451283814 |54514.617199066364|28  |\n",
            "|IRRELEVANT   |well      |11962 |4.329038044837416 |51783.953092345175|29  |\n",
            "|IRRELEVANT   |little    |11238 |4.478913637807677 |50334.03146168268 |30  |\n",
            "|RELEVANT     |great     |252982|2.339466177097054 |591842.832414367  |1   |\n",
            "|RELEVANT     |food      |231987|2.4882689796596167|577246.0557842955 |2   |\n",
            "|RELEVANT     |good      |209537|2.5770313395532987|539983.4157959796 |3   |\n",
            "|RELEVANT     |service   |158799|2.8056455539997467|445533.70832960575|4   |\n",
            "|RELEVANT     |place     |121819|2.9767423845341865|362623.78054157004|5   |\n",
            "|RELEVANT     |staff     |91472 |3.274291761802232 |299506.01603557373|6   |\n",
            "|RELEVANT     |friendly  |85843 |3.2975729159884994|283073.55182720075|7   |\n",
            "|RELEVANT     |always    |78726 |3.5321065001477563|278068.61633063224|8   |\n",
            "|RELEVANT     |nice      |74365 |3.4440469094848103|256116.54842383793|9   |\n",
            "|RELEVANT     |store     |56848 |4.010623399241094 |227995.91900005774|10  |\n",
            "|RELEVANT     |get       |58680 |3.839536536518086 |225304.00396288128|11  |\n",
            "|RELEVANT     |love      |60402 |3.689278847185948 |222839.82092772564|12  |\n",
            "|RELEVANT     |best      |59050 |3.7373919602300543|220692.9952515847 |13  |\n",
            "|RELEVANT     |time      |54839 |3.8951598854889107|213606.67296032637|14  |\n",
            "|RELEVANT     |go        |51534 |3.9065885813125334|201322.1359493601 |15  |\n",
            "|RELEVANT     |like      |47905 |4.035804915618521 |193335.23448270524|16  |\n",
            "|RELEVANT     |one       |47033 |4.093896967715667 |192548.25608257094|17  |\n",
            "|RELEVANT     |clean     |44103 |3.9683929866753864|175018.03589134457|18  |\n",
            "|RELEVANT     |prices    |38650 |4.096960251046184 |158347.51370293502|19  |\n",
            "|RELEVANT     |fast      |34575 |4.417216474041522 |152725.25958998562|20  |\n",
            "|RELEVANT     |back      |34957 |4.296863747514095 |150205.4660218502 |21  |\n",
            "|RELEVANT     |well      |34392 |4.329038044837416 |148884.27643804843|22  |\n",
            "|RELEVANT     |people    |35588 |4.156981775074119 |147938.66741133772|23  |\n",
            "|RELEVANT     |really    |34016 |4.323149503675753 |147056.2535170344 |24  |\n",
            "|RELEVANT     |pizza     |29628 |4.718943616328168 |139812.86146457097|25  |\n",
            "|RELEVANT     |order     |28682 |4.7553558188147464|136393.11559524457|26  |\n",
            "|RELEVANT     |excellent |31428 |4.314236269739693 |135587.81748537908|27  |\n",
            "|RELEVANT     |helpful   |30660 |4.318056095009093 |132391.5998729788 |28  |\n",
            "|RELEVANT     |little    |28260 |4.478913637807677 |126574.09940444496|29  |\n",
            "|RELEVANT     |selection |28741 |4.403057249475813 |126548.26840718434|30  |\n",
            "+-------------+----------+------+------------------+------------------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.window import Window\n",
        "\n",
        "w = Window.partitionBy(\"label\").orderBy(F.desc(\"score\"))\n",
        "top_uni = (tfidf_label\n",
        "           .withColumn(\"rank\", F.row_number().over(w))\n",
        "           .filter(F.col(\"rank\") <= 30)\n",
        "           .select(\"label\",\"term\",\"tf\",\"idf\",\"score\",\"rank\"))\n",
        "\n",
        "top_uni.orderBy(\"label\",\"rank\").show(120, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563a0e07",
      "metadata": {},
      "source": [
        "It is thus concluded that through statistical and pattern searching methods to label data, the model is not able to distinguish between relevant and irrelevant data. There are highly ranked words appearing in irrelevant and relevant datasets. Thus, with the overlap, the model is not able to fully ascertain if the reviews are relevant or not relevant. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e6614b1",
      "metadata": {},
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08c38a49",
      "metadata": {},
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4325a108",
      "metadata": {},
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fba26d",
      "metadata": {},
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400b2bac",
      "metadata": {},
      "source": [
        "This demo shall use three reviews for display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2516b94e",
      "metadata": {},
      "outputs": [],
      "source": [
        "!unzip -o bert_cls_model.zip -d ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f28d6b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "relevant_test = relevant_clean.withColumn(\"random_sort_key\", rand()).orderBy(\"random_sort_key\").drop(\"random_sort_key\").limit(3).withColumn(\"label\", lit(\"RELEVANT\")).select(\"text\", \"label\")\n",
        "irrelevant_test = irrelevant_clean.withColumn(\"random_sort_key\", rand()).orderBy(\"random_sort_key\").drop(\"random_sort_key\").limit(3).withColumn(\"label\", lit(\"IRRELEVANT\")).select(\"text\", \"label\")\n",
        "ads_test = ads_only.withColumn(\"random_sort_key\", rand()).orderBy(\"random_sort_key\").drop(\"random_sort_key\").limit(3).withColumn(\"label\", lit(\"ADVERTISEMENT\")).select(\"text\", \"label\")\n",
        "\n",
        "df_test = relevant_test.unionByName(irrelevant_test).unionByName(ads_test)\n",
        "df_in = df_test.select(F.col(\"text\").cast(\"string\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "409102e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "\n",
        "MODEL_DIR = \"./bert_cls_model\"   \n",
        "pipe = PipelineModel.load(MODEL_DIR)\n",
        "\n",
        "# Classifier output column (often \"class\")\n",
        "OUT_COL = pipe.stages[-1].getOutputCol()\n",
        "print(\"Classifier output column:\", OUT_COL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8c7326e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Keep both text and label when passing into pipeline\n",
        "df_in = df_test.select(F.col(\"text\").cast(\"string\"), \"label\")\n",
        "\n",
        "scored = pipe.transform(df_in)\n",
        "\n",
        "pred = (\n",
        "    scored\n",
        "    .select(\n",
        "        \"text\",    # original review text\n",
        "        \"label\",   # actual label\n",
        "        F.expr(f\"{OUT_COL}[0].result\").alias(\"pred_label\")  # predicted label\n",
        "    )\n",
        ")\n",
        "\n",
        "pred.show(truncate=50)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv_hackathon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
